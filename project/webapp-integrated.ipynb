{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.tokenize.api import StringTokenizer\n",
    "from collections import defaultdict, deque\n",
    "import re\n",
    "import itertools\n",
    "import jellyfish\n",
    "from pymining import itemmining, assocrules\n",
    "from functools import reduce\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "from oracle.modules.module import Module\n",
    "from oracle.modules.strikerate import StrikeRate\n",
    "from oracle.modules.obp import OBP\n",
    "from oracle.modules.leadrunnerthird import LeadRunnerOnThird\n",
    "from oracle.modules.leadrunnersecond import LeadRunnerOnSecond\n",
    "from oracle.modules.leadrunnerfirst import LeadRunnerOnFirst\n",
    "from oracle.modules.steal import Steal\n",
    "from oracle.modules.bunt import Bunt\n",
    "from oracle.modules.swing import Swing\n",
    "from oracle.modules.fpt import FPT\n",
    "from oracle.modules.fps import FPS\n",
    "from oracle.modules.power_sequence import PowerSequence\n",
    "from oracle.modules.zones import ClassifyZones\n",
    "from oracle.modules.flashcard import Flashcard\n",
    "from oracle.recommendations.recommendation import Recommendation, RecommendationItem\n",
    "from oracle.dknowledge import Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Oracle:\n",
    "\n",
    "    def __init__(self, config):\n",
    "        self.config = config\n",
    "\n",
    "    def is_number(self, s):\n",
    "        '''Return if a string can be parsed as a number'''\n",
    "        try:\n",
    "            float(s)\n",
    "            return True\n",
    "        except ValueError:\n",
    "            return False\n",
    "\n",
    "    def user_input(self, args):\n",
    "\n",
    "        '''Prompt user to clarify ambiguous tokens in the query'''\n",
    "\n",
    "        print('What do you mean by ' + \"\\'\" + args + \"\\'\")\n",
    "\n",
    "        feature = input('Relevant feature: ')  # prompt for feature\n",
    "        value = input('Associated value: ')  # prompt for value\n",
    "\n",
    "        return (feature, [value])\n",
    "\n",
    "    def identify_val_via_user(self, relevant, value):\n",
    "\n",
    "        '''Given a feature determined to be relevant, prompt user for its associated value if ambiguous'''\n",
    "\n",
    "        print('How does ' + \"\\'\" + value + \\\n",
    "              \"\\'\" + 'relate to the field ' + \"\\'\" + relevant + \"\\'\")\n",
    "        val = input('Relevant value: ')  # prompt for value\n",
    "\n",
    "        return [val]\n",
    "\n",
    "    def substitute_conjunction(self, c):\n",
    "\n",
    "        '''Return base Pandas substitution for conjunction'''\n",
    "\n",
    "        result = {'OR': '|', 'AND': '&', 'NOT': '!'}\n",
    "        return result[c]\n",
    "\n",
    "    def is_expr(self, entity):\n",
    "        '''Holds suite of pre built functions (averages, etc.)'''\n",
    "        return entity in self.config.module_keywords\n",
    "\n",
    "    def module_filter(self, module, params={}):\n",
    "        module = module()\n",
    "        iset = range(len(self.config.filtered.index))\n",
    "        rset = module.execute(iset, self.config.module_tables['filtered'], params, False)\n",
    "        self.config.filtered = self.config.filtered.iloc[rset, :]\n",
    "\n",
    "    def print_module_delegation(self, keyword, module):\n",
    "        print(\"delegating computation of '\" + keyword + \"' to module \" + str(module))\n",
    "\n",
    "    def react_conversion(self, plot_type, x, y):\n",
    "        data = {}\n",
    "        xs, ys = ['data1'], ['data2']\n",
    "        xs.extend(x)\n",
    "        ys.extend(y)\n",
    "        data['type'] = plot_type\n",
    "        result = [xs,ys]\n",
    "        data['columns'] = result\n",
    "        return data\n",
    "    \n",
    "    def parse_module_and_params(self, entity, extract=True):\n",
    "        parsed_module, parsed_param = entity, {}\n",
    "        if extract:\n",
    "            parsed_module = re.match('\\[(.*)\\]', entity)\n",
    "        if parsed_module is not None:\n",
    "            if extract:\n",
    "                parsed_module = parsed_module.groups()[0]\n",
    "            keyIndex = parsed_module.find('=')\n",
    "            if keyIndex != -1:\n",
    "                parsed_param = parsed_module[keyIndex+1:]\n",
    "                parsed_param = re.match('\\{\\s*(.*)\\s*\\}', parsed_param)\n",
    "                if parsed_param is None:\n",
    "                    parsed_param = {}\n",
    "                else:\n",
    "                    param_dict = {}\n",
    "                    parsed_param = parsed_param.groups()[0].strip()\n",
    "                    parsed_param = parsed_param.split(',')\n",
    "                    for pval in parsed_param:\n",
    "                        pair = pval.split('=')\n",
    "                        if len(pair) == 2:\n",
    "                            param_dict[pair[0]] = pair[1]\n",
    "                    parsed_param = param_dict\n",
    "                    parsed_module = parsed_module[:keyIndex]\n",
    "            else:\n",
    "                parsed_module = None\n",
    "        return parsed_module, parsed_param \n",
    "\n",
    "    def match_expr(self, entity, b_list, title, e_filters):\n",
    "\n",
    "        '''driver for filtering on pre-built functions'''\n",
    "        print(entity)\n",
    "\n",
    "        plot, status = None, True\n",
    "        result, names, plot_l = '', [], []\n",
    "        module = None\n",
    "        self.config.module_tables = {'filtered': self.config.filtered, 'time_series': self.config.filtered}\n",
    "\n",
    "        if b_list is not None:  # if b_list is not empty, perform a plot over distribution of feature\n",
    "            dist_plot = True\n",
    "            result = {}\n",
    "            for name, group in self.config.filtered:  # iterate over each group\n",
    "                calc = None\n",
    "                parsed_module, parsed_param = self.parse_module_and_params(entity, False)\n",
    "                if parsed_module is not None:\n",
    "                    if parsed_module in config.special_modules:\n",
    "                        return None, None, False\n",
    "                    entity = parsed_module\n",
    "                if entity in self.config.module_keywords:\n",
    "                    module = self.config.module_keywords[entity]() if module is None else module\n",
    "                    print('entity: ' + entity + ' matched to module: ' + str(module))\n",
    "                    calc = module.execute(self.config.filtered.indices[name],\n",
    "                                          self.config.module_tables['filtered'], parsed_param, True)\n",
    "                if calc is not None:\n",
    "                    if (entity not in e_filters or (calc.shape == (1, 1) and\n",
    "                                                    (re.search(r'(?i)^over$', e_filters[entity][0]) is not None\n",
    "                                                     and calc.iloc[0, 0] > float(e_filters[entity][1])) or\n",
    "                                                    (re.search(r'(?i)^under$', e_filters[entity][0]) is not None\n",
    "                                                     and calc.iloc[0, 0] < float(e_filters[entity][1])))):\n",
    "                        names.append(name)  # add to x labels list\n",
    "                        result[name] = calc\n",
    "                        if dist_plot:\n",
    "                            calc = calc.iloc[0, 0]\n",
    "                            plot_l.append(calc)  # add to y values list\n",
    "                        else:\n",
    "                            dist_plot = False\n",
    "\n",
    "            if plot_l and dist_plot:  # if calculated results are returned, generate the plot\n",
    "                # plot = self.react_conversion('bar', names, plot_l)\n",
    "                plot_data = pd.DataFrame({'x': names, 'y': plot_l})\n",
    "                status = False  # set time_series plot status to False\n",
    "\n",
    "        else:  # otherwise plot time series by default\n",
    "\n",
    "            parsed_module, parsed_param = self.parse_module_and_params(entity, False)\n",
    "            if parsed_module is not None:\n",
    "                entity = parsed_module\n",
    "            if entity in self.config.module_keywords:\n",
    "                module = self.config.module_keywords[entity]()\n",
    "                iset = range(len(self.config.filtered.index))\n",
    "                print('entity: ' + entity + ' matched to module: ' + str(module))\n",
    "                result = module.execute(iset, self.config.module_tables['filtered'], parsed_param, False)\n",
    "                self.config.time_series = self.config.filtered.groupby('Date')  # group filtered dataset by date\n",
    "                if result is not None:\n",
    "                    if isinstance(result, str):\n",
    "                        return result, None, False\n",
    "                    elif result.shape == (1,1):\n",
    "                        for name, group in self.config.time_series:\n",
    "                            module = self.config.module_keywords[entity]() if module is None else module\n",
    "                            calc = module.execute(self.config.time_series.indices[name],\n",
    "                                                  self.config.module_tables['time_series'], False)\n",
    "                            calc = calc.iloc[0, 0]\n",
    "                            if calc is not None:\n",
    "                                names.append(name)\n",
    "                                plot_l.append(calc)\n",
    "                else:\n",
    "                    return None, None, False\n",
    "\n",
    "                if plot_l:\n",
    "                    # plot time series chart\n",
    "                    names = [str(name) for name in names]\n",
    "                    plot_data = pd.DataFrame({'x': names, 'y': plot_l})\n",
    "\n",
    "        return result, plot_data, status\n",
    "\n",
    "    def sequential_entity(self, e_list, b_feat, actors):\n",
    "\n",
    "        '''Perform sequence of filter reductions on the entity list'''\n",
    "\n",
    "        response = str(' '.join([re.findall(r'^\\(?(.+[^)])\\)?', actor[0])[0] for actor in actors])).title()\n",
    "        criteria, added, expr = self.generate_criteria(e_list)  # generate entity criteria (feature, [values])\n",
    "        result = None\n",
    "\n",
    "        eHash = {}\n",
    "        for eFilter in self.config.entityFilters:\n",
    "            eFilterIndex = self.config.qry.find(eFilter[0] + ' ' + eFilter[1])\n",
    "            minDiff = None\n",
    "            for e in expr:\n",
    "                eExprIndex = self.config.qry.find(e)\n",
    "                if minDiff is None or (eFilterIndex - eExprIndex) < minDiff:\n",
    "                    minDiff = eFilterIndex - eExprIndex\n",
    "                    eHash[eFilter] = e\n",
    "\n",
    "        eHash = {v: k for k, v in eHash.items()}\n",
    "\n",
    "        for i in range(len(criteria)):  # sequentially filter database on entity criteria\n",
    "\n",
    "            exec_str = '(' + self.perform_filter(self.config.FILTER, criteria[i], b_feat) + ')'  # perform filter\n",
    "\n",
    "            if not b_feat:\n",
    "                exec_str = \"self.config.filtered = self.config.filtered[({0})]\".format(exec_str)\n",
    "            else:\n",
    "                exec_str = \"self.config.filtered = self.config.filtered.apply(lambda g: ({0}))\".format(exec_str)\n",
    "            print('\\n' + exec_str)\n",
    "            exec(exec_str)\n",
    "\n",
    "            if b_feat:\n",
    "                self.config.filtered = self.config.filtered.groupby(b_feat)\n",
    "\n",
    "            feat = criteria[i][0]\n",
    "            self.config.MOST_RECENT_QUERY.add(feat)  # cache this query as most recent\n",
    "            self.config.ITEMSETS[len(self.config.ITEMSETS) - 1].add(feat)  # add to frequent itemsets cache\n",
    "            self.config.EXP_DECAY[feat] = 1  # set distance of feature since last fetched to 1\n",
    "\n",
    "            for j in range(i, len(criteria)):\n",
    "                self.config.COOCCURENCE_HASH[feat][criteria[j][0]] += 1  # update cooccurrence hash\n",
    "\n",
    "        for e in expr:\n",
    "            # sequentially execute all user defined actions\n",
    "            result, plot_data, status = self.match_expr(e, b_feat, response, eHash)\n",
    "\n",
    "        for feat in added:  # restore original columns\n",
    "            del self.config.filtered[feat]\n",
    "            del self.config.DOMAIN_KNOWLEDGE[feat]\n",
    "\n",
    "        self.config.ITEMSETS[len(self.config.ITEMSETS) - 1] = tuple(self.config.ITEMSETS[len(self.config.ITEMSETS) - 1])\n",
    "\n",
    "        sample_size, plot_type = None, None\n",
    "        if result is not None and not isinstance(result, str):\n",
    "            if status:  # output textual response\n",
    "                if result.shape == (1, 1):\n",
    "                    result.columns = ['Output']\n",
    "                    sample_size = pd.DataFrame([self.config.filtered.shape[0]])\n",
    "                    plot_type = 'time-series'\n",
    "            else:\n",
    "                appended = pd.DataFrame()\n",
    "                names = []\n",
    "                for name in result:\n",
    "                    if result[name].shape[1] == 1:\n",
    "                        result[name].columns = ['Output']\n",
    "                    appended = appended.append(result[name])\n",
    "                    names.append(name)\n",
    "                appended['name'] = names\n",
    "                appended.columns = ['Output', 'name']\n",
    "                result = appended\n",
    "                sample_size = self.config.filtered.size().to_frame()\n",
    "                plot_type = 'bar'\n",
    "            sample_size.columns = ['Sample Sizes']\n",
    "        elif result:\n",
    "            plot_data, plot_type = None, 'png' \n",
    "        else:\n",
    "            print('\\n The output of your query resulted in a sample size of 0.')\n",
    "            return None, None, None, None, None\n",
    "\n",
    "        rterms = self.generate_features_rf_()\n",
    "\n",
    "        if rterms:\n",
    "            print('\\n' + '(Relevance Feedback) Investigate Features More Like This: ' + str(rterms) + '\\n')\n",
    "        \n",
    "        return result, sample_size, plot_data, plot_type, rterms\n",
    "\n",
    "    def generate_criteria(self, e_list):\n",
    "\n",
    "        '''Parse each entity token in the query into list of (feature, [value])'''\n",
    "\n",
    "        entities = []\n",
    "\n",
    "        for i in range(len(e_list)):  # iterate over entity list\n",
    "            if e_list[i] == self.config.GENITIVE:\n",
    "                continue\n",
    "            entities.append(re.findall(r'^\\(?(.+[^)])\\)?', e_list[i])[0])\n",
    "\n",
    "        prev_feat = None\n",
    "        criteria, added, expr = [], [], []\n",
    "        rec_item = self.config.RECOMMENDATION.data[self.config.qry]  # hash current query into RECOMMENDATIONS hash\n",
    "        conj = None\n",
    "\n",
    "        for entity in entities:\n",
    "            parsed_param = re.match('\\[(.*)\\]', entity)\n",
    "            if parsed_param is not None:\n",
    "                parsed_param = parsed_param.groups()[0]\n",
    "                expr.append(parsed_param)\n",
    "                continue\n",
    "            elif self.is_expr(entity):  # check if token is a pre-built expression\n",
    "                expr.append(entity)\n",
    "                continue\n",
    "            elif self.config.is_conj(entity):  # check if token is a conjunction and tag it\n",
    "                conj = entity\n",
    "                continue\n",
    "\n",
    "            curr_feat = self.match_(entity)  # extract relevant feature\n",
    "            start = self.config.qry.find(entity)\n",
    "            end = start + len(entity)\n",
    "            rec_item.index_hash[curr_feat] = (start, end)\n",
    "            self.config.RECOMMENDATION.data[self.config.qry] = rec_item\n",
    "\n",
    "            if curr_feat is None:\n",
    "                print(curr_feat + \"generate_criteria\")\n",
    "                return self.user_input(entity)  # prompt user to clarify input\n",
    "            else:\n",
    "                # extract associated value to relevant feature\n",
    "                val = self.match_arg_to_feature_value(curr_feat, entity)\n",
    "\n",
    "                while val is None:\n",
    "                    # prompt user if argument specification is ambiguous\n",
    "                    val = self.identify_val_via_user(curr_feat, val)\n",
    "\n",
    "                if criteria:  # handle conjunctions in query\n",
    "                    if conj == 'OR':  # union logic\n",
    "                        if criteria[len(criteria) - 1][1] == curr_feat:\n",
    "                            criteria[len(criteria) - 1][1][0] += '+' + '+'.join(val)\n",
    "                        else:\n",
    "                            pass\n",
    "                    elif conj == 'NOT':  # negation logic\n",
    "                        criteria.extend([(curr_feat, ['!' + v_key]) for v_key in val])\n",
    "                    else:\n",
    "                        criteria.append((curr_feat, ['+'.join(val)]))\n",
    "                else:  # intersection logic\n",
    "                    criteria.append((curr_feat, ['+'.join(val)]))\n",
    "                conj = None\n",
    "\n",
    "        features = [feat for feat in self.config.RECOMMENDATION.data[self.config.qry].index_hash]\n",
    "        features.sort()  # sort features to specify common key\n",
    "        self.config.RECOMMENDATIONS[str(features)].append(self.config.RECOMMENDATION)  # hash features in RECOMMENDATIONS\n",
    "\n",
    "        return criteria, added, expr\n",
    "\n",
    "    def sequential_filter(self, f_list):\n",
    "\n",
    "        '''Execute sequential filters over dataset on supplied filtering criteria'''\n",
    "\n",
    "        self.config.MOST_RECENT_QUERY = set()  # reinitialize MOST_RECENT_QUERY\n",
    "        self.config.ITEMSETS.append(set())  # reinitialize ITEMSETS\n",
    "\n",
    "        for feat in self.config.EXP_DECAY:\n",
    "            self.config.EXP_DECAY[feat] += 1  # increment all features distance from being last fetched by 1\n",
    "\n",
    "        self.config.filtered = self.config.X.copy()  # reinitialize filtered\n",
    "        self.config.time_series = self.config.filtered\n",
    "        self.config.module_tables = {'filtered': self.config.filtered, 'time_series': self.config.time_series}\n",
    "        filters = self.feature_assoc_filters_helper(f_list) # generate filtering criteria as [[(feature, [values])]]\n",
    "        grouped, b_list = None, []\n",
    "        for f_key in filters:\n",
    "            exec_str = ''\n",
    "            for i in range(len(f_key)):  # iterate over each filtering criterion\n",
    "                f_, c_, conj_ = f_key[i][0], f_key[i][1], f_key[i][2] # filtering operation, criteria, join flag\n",
    "                if c_ is None: \n",
    "                    continue\n",
    "                feat = c_[0]\n",
    "                if type(feat) is not dict:\n",
    "                    self.config.MOST_RECENT_QUERY.add(feat)\n",
    "                    self.config.ITEMSETS[len(self.config.ITEMSETS) - 1].add(feat)  # update ITEMSETS\n",
    "                    self.config.EXP_DECAY[feat] = 1  # set distance since being fetched to 1\n",
    "                    for j in range(i, len(f_key)):\n",
    "                        self.config.COOCCURENCE_HASH[feat][f_key[j][1][0]] += 1  # update cooccurence hash\n",
    "                    if not c_[1]:\n",
    "                        b_list.append(feat)  # add to b_list if filtering criteria is over a feature's entire distribution\n",
    "                        continue\n",
    "                    if conj_ is not None:\n",
    "                        conj_ = self.substitute_conjunction(conj_)\n",
    "                        exec_str += \" {0} \".format(conj_)\n",
    "                if c_[1] in self.config.module_keywords.values():\n",
    "                    params = {} if c_[0] is None else feat\n",
    "                    self.print_module_delegation(str(feat), str(c_[1]))\n",
    "                    self.module_filter(c_[1], params)\n",
    "                elif type(feat) is not dict:\n",
    "                    exec_str += self.perform_filter(f_, c_, False)\n",
    "            if exec_str:\n",
    "                exec_str = '(' + exec_str + ')'\n",
    "                exec_str = \"self.config.filtered = self.config.filtered[({0})]\".format(exec_str)\n",
    "                print('\\n' + exec_str)\n",
    "                exec(exec_str)\n",
    "\n",
    "        if b_list:\n",
    "            grouped, b_feat = self.by_helper_([b_list, []])\n",
    "        else:\n",
    "            for module in self.config.clookup:\n",
    "                if module().set_module():\n",
    "                    keyword = list(module().get_lexicon())[0]\n",
    "                    self.print_module_delegation(keyword, str(module))\n",
    "                    self.module_filter(module)\n",
    "\n",
    "        return (self.config.filtered, None) if grouped is None else (grouped, b_feat)\n",
    "\n",
    "    def check_filters(self, fname):\n",
    "        max_conf, max_filt = 0, None\n",
    "\n",
    "        for entry in self.config.module_keywords:\n",
    "            conf_f = jellyfish.jaro_distance(fname, entry)\n",
    "            if conf_f > max_conf:\n",
    "                max_conf, max_filt = conf_f, self.config.module_keywords[entry]\n",
    "\n",
    "        if max_conf > self.config.CONF_THRESHOLD:\n",
    "            return max_filt\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "    def feature_assoc_filters_helper(self, filters):\n",
    "\n",
    "        '''Return list of (relevant feature, [associated values]) tuples to filter automatically'''\n",
    "\n",
    "        f_ = []  # initialize result list\n",
    "        featureHash = {}\n",
    "        self.config.RECOMMENDATION = Recommendation(self.config.qry)  # initialize RECOMMENDATION to hash on current query\n",
    "        rec_item = RecommendationItem()\n",
    "        self.config.entityFilters, prev_numeric, numeric_token = [], [], None\n",
    "\n",
    "        for f_item in filters:\n",
    "\n",
    "            tokens = re.findall(r'\\(.+?\\)|AND|OR|NOT', f_item)  # tokenize filters\n",
    "            tokens[0] = re.findall(r'%(.+)%', tokens[0])[0]\n",
    "            tokens[1] = tokens[1][1:]\n",
    "            tokens = [re.findall(r'\\((.+?)\\)', token)[0] if '(' in token else token for token in tokens]\n",
    "            prev_feat, relevant, val, cat, joined = None, None, None, '', []\n",
    "            i, inc, c_flag = 1, False, False\n",
    "            negated = None\n",
    "\n",
    "            cset = self.config.clookup.values()\n",
    "            cset = [k for ckey in cset for k in ckey]\n",
    "\n",
    "            while i < len(tokens):  # iterate over each filter in the filtered list\n",
    "\n",
    "                token = tokens[i]\n",
    "                token = token.split()\n",
    "                count = 0\n",
    "\n",
    "                for t in token:\n",
    "                    for ckey in cset:\n",
    "                        jaro = jellyfish.jaro_distance(t, ckey)\n",
    "                        if jaro > self.config.CONF_THRESHOLD:\n",
    "                            count += 1\n",
    "\n",
    "                if count == len(token):\n",
    "                    f_.append([(tokens[0], None, None)])\n",
    "                    i += 1\n",
    "                    continue\n",
    "\n",
    "                is_filter = False\n",
    "\n",
    "                if tokens[i].startswith('NOT'):\n",
    "                    cat += tokens[i]\n",
    "                    negated = re.findall(r'NOT|.+', tokens[i])[1]\n",
    "                    relevant = self.match_(negated)\n",
    "                    if relevant is None:\n",
    "                        relevant, val = self.user_input(negated)\n",
    "\n",
    "                if negated is None:\n",
    "\n",
    "                    if tokens[0] in self.config.numericFilters:\n",
    "                        if not prev_numeric:\n",
    "                            self.config.entityFilters.append((tokens[0], tokens[i]))\n",
    "                            break\n",
    "                        else:\n",
    "                            relevant = prev_numeric.pop()\n",
    "                            val = [tokens[i]]  # set argument value to match token if a numeric feature\n",
    "\n",
    "                    if relevant is None:\n",
    "                        relevant = self.match_(tokens[i])  # extract relevant feature for tokens[i]\n",
    "                        if relevant is None:\n",
    "                            filter_to_apply, potential_value = self.parse_module_and_params(tokens[i])\n",
    "                            if filter_to_apply is not None:\n",
    "                                filter_to_apply = self.check_filters(filter_to_apply)\n",
    "                            else:\n",
    "                                potential_value = None\n",
    "                            if filter_to_apply is None:\n",
    "                                relevant, val = self.user_input(tokens[i])  # prompt user if ambiguous\n",
    "                            else:\n",
    "                                is_filter = True\n",
    "                                if potential_value is None:\n",
    "                                    f_.append([(tokens[0], (tokens[i], filter_to_apply), None)])\n",
    "                                else:\n",
    "                                    f_.append([(tokens[0], (potential_value, filter_to_apply), None)])\n",
    "                        if not is_filter:\n",
    "                            start = self.config.qry.find(tokens[i])  # start index stored before cached in RECOMMENDATION\n",
    "                            end = start + len(tokens[i])  # end index stored before cached in RECOMMENDATION\n",
    "                            rec_item.index_hash[relevant] = (start, end)\n",
    "\n",
    "                            # set current recommendation item as the value to RECOMMENDATION hashed on the current query\n",
    "                            self.config.RECOMMENDATION.data[self.config.qry] = rec_item\n",
    "\n",
    "                    if val is None and not is_filter:\n",
    "\n",
    "                        cat += tokens[i]\n",
    "                        joined.append(tokens[i])\n",
    "\n",
    "                        while (i + 1) < len(tokens) and self.config.is_conj(\n",
    "                                tokens[i + 1].lower()):  # check if next token is conjunction\n",
    "                            # otherwise append the next feature value to the running result\n",
    "                            cat += tokens[i + 1] + tokens[i + 2]\n",
    "                            joined.append(tokens[i + 2])\n",
    "                            i += 3\n",
    "\n",
    "                        for core_entity in self.config.CORE:\n",
    "                            if jellyfish.jaro_distance(core_entity, cat) > self.config.NAME_THRESHOLD:\n",
    "                                c_flag = True\n",
    "                                break\n",
    "\n",
    "                if not c_flag:\n",
    "\n",
    "                    if not is_filter:\n",
    "\n",
    "                        if val is None:\n",
    "\n",
    "                            # call subroutine to generate filter criteria for current (feature, [arguments]) pair\n",
    "                            result = self.generate_filter_criteria(cat, relevant)\n",
    "                            inserted = False\n",
    "\n",
    "                            if not result[1] or result[1][0] != 'is.numeric':\n",
    "                                for joinedEntry in joined:\n",
    "                                    if (joinedEntry in self.config.conjunctive and\n",
    "                                            self.config.conjunctive[joinedEntry][1] in featureHash):\n",
    "                                        stored = self.config.conjunctive[joinedEntry]\n",
    "                                        if not inserted:\n",
    "                                            f_[featureHash[stored[1]]].append((tokens[0], result, stored[0]))\n",
    "                                            inserted = True\n",
    "                                        featureHash[joinedEntry] = featureHash[stored[1]]\n",
    "\n",
    "                                if not inserted:\n",
    "                                    f_.append([(tokens[0], result, None)])  # append to result list\n",
    "                                    for joinedEntry in joined:\n",
    "                                        featureHash[joinedEntry] = len(f_) - 1\n",
    "\n",
    "                            else:\n",
    "                                numeric_token = cat\n",
    "                                prev_numeric.append(result[0])\n",
    "\n",
    "                        elif numeric_token is not None:\n",
    "                            if val[0] in self.config.conjunctive and self.config.conjunctive[val[0]][1] in featureHash:\n",
    "                                stored = self.config.conjunctive[val[0]]\n",
    "                                f_[featureHash[stored[1]]].append((tokens[0], (relevant, val), stored[0]))\n",
    "                                featureHash[val[0]] = featureHash[stored[1]]\n",
    "                            elif numeric_token in self.config.conjunctive and self.config.conjunctive[numeric_token][\n",
    "                                1] in featureHash:\n",
    "                                stored = self.config.conjunctive[numeric_token]\n",
    "                                f_[featureHash[stored[1]]].append((tokens[0], (relevant, val), stored[0]))\n",
    "                                featureHash[numeric_token] = featureHash[stored[1]]\n",
    "                            else:\n",
    "                                f_.append([(tokens[0], (relevant, val), None)])  # append to result list\n",
    "                                featureHash[val[0]] = len(f_) - 1\n",
    "\n",
    "                            numeric_token = None\n",
    "\n",
    "                if not inc:\n",
    "                    i += 1\n",
    "\n",
    "                inc, c_flag, val, cat, joined = False, False, None, '', []\n",
    "\n",
    "        return f_\n",
    "\n",
    "    def perform_filter(self, f_key, c_key, b_list):\n",
    "\n",
    "        '''Perform relevant filter'''\n",
    "\n",
    "        result = {\n",
    "            'filter': lambda D: self.filter_helper_(c_key, b_list),\n",
    "            'by': lambda D: self.by_helper_(c_key),\n",
    "            'over': lambda D: self.over_helper_(c_key),\n",
    "            'under': lambda D: self.under_helper_(c_key),\n",
    "            'between': lambda D: self.between_helper_(c_key),\n",
    "            'except': lambda D: self.except_helper_(c_key),\n",
    "            'near': lambda D: self.near_helper_(c_key),\n",
    "            'until': lambda D: self.until_helper_(c_key),\n",
    "            'to': lambda D: self.to_helper_(c_key),\n",
    "            'after': lambda D: self.after_helper_(c_key),\n",
    "            'before': lambda D: self.before_helper_(c_key),\n",
    "            'against': lambda D: self.compare_helper_(c_key)\n",
    "        }[f_key](self.config.filtered)\n",
    "\n",
    "        return result;\n",
    "\n",
    "    def generate_filter_criteria(self, args, hint=None):\n",
    "\n",
    "        '''Return relevant (feature, [values]) tuple that matches argument'''\n",
    "\n",
    "        c_keys = ['AND', 'OR', 'NOT']\n",
    "        pat = '((?:' + '|'.join(c_keys) + '))'\n",
    "        c_list = re.split(pat, args)  # split on conjunctions\n",
    "        c_list = [t for t in c_list if t]\n",
    "\n",
    "        if hint is None:\n",
    "            print(args + \"generate_filter_criteria\")\n",
    "            return self.user_input(args)  # if ambiguous, prompt user to specify\n",
    "        else:\n",
    "            relevant = hint\n",
    "\n",
    "        print('\\nfeature association: most relevant feature for arg (', args, ') is ' + relevant)\n",
    "\n",
    "        if relevant in self.config.name_ids and args in self.config.IDENTIFIERS[relevant]:\n",
    "            return (relevant, [args])\n",
    "\n",
    "        if 'is.numeric' in list(self.config.DOMAIN_KNOWLEDGE[relevant].values())[0]:\n",
    "            if not self.is_number(args):\n",
    "                test_split = re.split(pat, args)\n",
    "                criteria = self.generate_filter_helper_(relevant, test_split)\n",
    "                return (relevant, criteria)\n",
    "            else:\n",
    "                return (relevant, ['is.numeric'])\n",
    "\n",
    "        featureDist = [self.config.FEATURE_DIST[feat] for feat in self.config.FEATURE_DIST\n",
    "                       if jellyfish.jaro_distance(args, feat) > self.config.CONF_THRESHOLD]\n",
    "        if featureDist:\n",
    "            return (featureDist[0], [])\n",
    "\n",
    "        criteria = self.generate_filter_helper_(relevant, c_list)\n",
    "\n",
    "        return (relevant, criteria)\n",
    "\n",
    "    def generate_filter_helper_(self, relevant, c_list):\n",
    "\n",
    "        criteria = []\n",
    "        conj, unionIndex = None, 0\n",
    "\n",
    "        for term in c_list:\n",
    "\n",
    "            if self.config.is_conj(term.lower()):\n",
    "                conj = term\n",
    "            else:\n",
    "                lookup = re.match('(.+)', term).group()  # extract token\n",
    "                val = self.match_arg_to_feature_value(relevant, lookup)  # look up associated filter criteria on feature\n",
    "                print('generate_criteria: ' + str(val))\n",
    "\n",
    "                while val is None:\n",
    "                    val = self.identify_val_via_user(relevant, val)  # prompt user if associated value is ambiguous\n",
    "\n",
    "                if conj == 'NOT':  # handle negation logic\n",
    "                    negated = ['!' + v_key for v_key in val]\n",
    "                    criteria.extend(negated)\n",
    "                    unionIndex = len(criteria) - len(negated)\n",
    "                elif criteria:\n",
    "                    if conj == 'OR' or len(val) > 1:  # handle union logic\n",
    "                        for i in range(unionIndex, len(criteria)):\n",
    "                            criteria[i] += '+' + '+'.join(val)\n",
    "                        unionIndex = len(criteria) - 1\n",
    "                    else:\n",
    "                        criteria.append('+'.join(val))\n",
    "                        unionIndex = len(criteria) - 1\n",
    "                else:  # handle intersection logic\n",
    "                    criteria.append('+'.join(val))\n",
    "                    unionIndex = len(criteria) - 1\n",
    "\n",
    "                conj = None\n",
    "\n",
    "        return criteria\n",
    "\n",
    "    def filter_helper_(self, c_key, b_list):\n",
    "\n",
    "        '''Perform filter on categorical feature'''\n",
    "\n",
    "        feature = c_key[0]  # extract feature\n",
    "        args = c_key[1]  # extract relevant values of feature to filter on\n",
    "        exec_str = ''\n",
    "        print('(' + str(feature) + ', ' + str(args) + '): filter_helper_')\n",
    "\n",
    "        if b_list:  # handle automated filtering over distribution of a feature\n",
    "\n",
    "            for index, f_tok in enumerate(args):  # iterate over each argument\n",
    "\n",
    "                f_ = re.split(r'(?:\\+|!)', f_tok)  # split by filters handled with conjunctive logic\n",
    "\n",
    "                if exec_str:\n",
    "                    exec_str += ' | '\n",
    "                if '!' in f_tok:  # handle negation\n",
    "                    exec_str += \"(g[g[\\'\" + feature + \"\\'] != \\'\" + f_[1] + \"\\'])\"\n",
    "                else:\n",
    "                    union = [item for item in f_]\n",
    "                    exec_str += \"(g[g[\\'\" + feature + \"\\'].isin(\" + str(union) + \")])\"\n",
    "\n",
    "        else:\n",
    "\n",
    "            for index, f_tok in enumerate(args):  # iterate over each argument\n",
    "\n",
    "                f_ = re.split(r'(?:\\+|!)', f_tok)  # split on conjunctions\n",
    "\n",
    "                if exec_str:\n",
    "                    exec_str += ' | '\n",
    "                if '!' in f_tok:  # negation logic\n",
    "                    exec_str += \"(self.config.filtered[\\'\" + feature + \"\\'] != \\'\" + f_[1] + \"\\')\"\n",
    "                    continue\n",
    "                exec_str += \"(self.config.filtered[\\'\" + feature + \"\\'] == \\'\" + f_[0] + \"\\')\"\n",
    "\n",
    "                for i in range(1, len(f_)):\n",
    "                    exec_str += \" | (self.config.filtered[\\'\" + feature + \"\\'] == \\'\" + f_[i] + \"\\')\"  # handle union logic\n",
    "\n",
    "        return exec_str\n",
    "\n",
    "    def by_helper_(self, c_key):\n",
    "\n",
    "        '''Filter database over distribution of a feature'''\n",
    "\n",
    "        features = c_key[0]  # extract features\n",
    "        self.config.filtered = self.config.filtered.groupby(features)  # group by features\n",
    "\n",
    "        return self.config.filtered, features\n",
    "\n",
    "    def over_helper_(self, c_key):\n",
    "\n",
    "        '''Filter database on values over a threshold for a feature'''\n",
    "\n",
    "        feature = c_key[0]  # extract feature\n",
    "        args = c_key[1]  # extract value to filter on\n",
    "        exec_str = \"(self.config.filtered[\\'\" + feature + \"\\'] > \" + args[0] + \")\"\n",
    "\n",
    "        return exec_str\n",
    "\n",
    "    def under_helper_(self, c_key):\n",
    "\n",
    "        '''Filter database on values under a threshold for a feature'''\n",
    "\n",
    "        feature = c_key[0]  # extract feature\n",
    "        args = c_key[1]  # extract value to filter on\n",
    "        exec_str = \"(self.config.filtered[\\'\" + feature + \"\\'] < \" + args[0] + \")\"\n",
    "\n",
    "        return exec_str\n",
    "\n",
    "    def between_helper_(self, c_key):\n",
    "\n",
    "        '''Filter database on values between two thresholds for a feature'''\n",
    "\n",
    "        feature = c_key[0]  # extract feature\n",
    "        args = c_key[1]  # extract value to filter on\n",
    "        left, right = args[0], args[1]\n",
    "        exec_str = \"(self.config.filtered[\\'\" + feature + \"\\'] > \" + left + \")\" \\\n",
    "                                                                       \"& (self.config.filtered[\\'\" + feature + \"\\'] < \" + right + \")\"\n",
    "\n",
    "        return exec_str\n",
    "\n",
    "    def except_helper_(self, c_key):\n",
    "\n",
    "        '''Filter database on values of a categorical feature except for those specified in the argument'''\n",
    "\n",
    "        feature = c_key[0]  # extract feature\n",
    "        args = c_key[1]  # extract values to filter on\n",
    "        unique_vals = list(set(list(self.config.filtered[feature].unique())) - set([args]))\n",
    "        unique_vals = [x for x in unique_vals if x == x]\n",
    "        unique_vals = '+'.join(unique_vals)\n",
    "\n",
    "        return self.filter_helper_([feature, unique_vals], None)\n",
    "\n",
    "    def near_helper_(self, c_key):\n",
    "\n",
    "        '''Filter database on values within +/- 0.5 std of the argument on a numerical feature'''\n",
    "\n",
    "        feature = c_key[0]  # extract relevant feature\n",
    "        args = c_key[1]  # extract values to filter on\n",
    "        left = str(float(args[0]) - 0.5 * self.config.filtered[feature].std())  # left bound -0.5 std\n",
    "        right = str(float(args[1]) + 0.5 * self.config.filtered[feature].std())  # right bound +0.5 std\n",
    "\n",
    "        if left > right:\n",
    "            left, right = right, left\n",
    "\n",
    "        return self.between_helper_([feature, [left, right]])\n",
    "\n",
    "    def match_(self, args):\n",
    "\n",
    "        '''Return relevant feature to filter on'''\n",
    "\n",
    "        featureDist = [self.config.FEATURE_DIST[feat] for feat in self.config.FEATURE_DIST\n",
    "                       if jellyfish.jaro_distance(args, feat) > self.config.CONF_THRESHOLD]\n",
    "        if featureDist:\n",
    "            return featureDist[0]\n",
    "\n",
    "        found_batter, found_pitcher = False, False\n",
    "        for identifier in self.config.IDENTIFIERS:\n",
    "            if args in self.config.IDENTIFIERS[identifier]:\n",
    "                if identifier == self.config.name_ids[0]:\n",
    "                    found_batter = True\n",
    "                else:\n",
    "                    found_pitcher = True\n",
    "\n",
    "        if not (found_batter and found_pitcher):\n",
    "            if found_batter:\n",
    "                return (self.config.name_ids[0])\n",
    "            elif found_pitcher:\n",
    "                return (self.config.name_ids[1])\n",
    "        else:\n",
    "            return ((self.config.name_ids[0], [args])\n",
    "                    if len(self.config.X[self.config.X[self.config.name_ids[0]] == args]) > len(\n",
    "                self.config.X[self.config.X[self.config.name_ids[1]] == args])\n",
    "                    else (self.config.name_ids[1]))\n",
    "\n",
    "        max_conf, max_feat = 0, ''\n",
    "\n",
    "        for entry in self.config.DOMAIN_KNOWLEDGE:  # iterate over terms in the system's domain knowledge\n",
    "\n",
    "            if args in self.config.DOMAIN_KNOWLEDGE[entry]:  # if term matches exactly return it\n",
    "                return entry\n",
    "\n",
    "            tokens = args.split()  # otherwise split tokens and accumulate evidence of belonging to each feature\n",
    "            conf_f = 0  # confidence\n",
    "            skipped = 0  # tokens not considered in parsing of the feature\n",
    "\n",
    "            for token in tokens:  # iterate over each token\n",
    "\n",
    "                if not self.is_number(token):\n",
    "\n",
    "                    curr = 0\n",
    "                    skipped += 1\n",
    "\n",
    "                    for desc in list(\n",
    "                            self.config.DOMAIN_KNOWLEDGE[entry].keys()):  # iterate over each term in domain knowledge\n",
    "\n",
    "                        # compute string similarity of query vs term in DOMAIN_KNOWLEDGE\n",
    "                        curr = max(jellyfish.jaro_distance(desc, token), curr)  # string similarity by jaro_distance\n",
    "\n",
    "                    conf_f += curr  # accumulate confidence score for feature\n",
    "\n",
    "            conf_f = conf_f / skipped if skipped > 0 else 0  # scaled confidence level of feature match\n",
    "\n",
    "            if conf_f > max_conf:  # update max confidence level and associated feature\n",
    "                max_conf, max_feat = conf_f, entry\n",
    "\n",
    "        if max_conf > self.config.CONF_THRESHOLD:\n",
    "            return max_feat  # return result\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "    def match_arg_to_feature_value(self, feature, args):\n",
    "\n",
    "        '''Return list of relevant arguments that match feature value'''\n",
    "        print('(' + feature + ', ' + args + '): ' + 'match_arg_to_feature_value')\n",
    "\n",
    "        if feature in self.config.name_ids:\n",
    "            return [args]\n",
    "\n",
    "        unique_vals = list(set(list(itertools.chain.from_iterable(self.config.DOMAIN_KNOWLEDGE[feature].values()))))\n",
    "        vals = {key: 0 for key in unique_vals}  # initialize hash to accumulate evidence for each value of feature\n",
    "        tokens = args.split()  # tokenize argument\n",
    "        max_val, max_arg = 0, []\n",
    "\n",
    "        for token in tokens:  # iterate over each token\n",
    "\n",
    "            if self.is_number(token):\n",
    "                return [token]\n",
    "\n",
    "            for lookup in self.config.DOMAIN_KNOWLEDGE[feature]:  # lookup relevant modifiers in domain knowledge\n",
    "\n",
    "                jaro = jellyfish.jaro_distance(token, lookup)  # compute string similarity of modifier vs query token\n",
    "\n",
    "                # jaro normalized as a confidence between [0, 1]\n",
    "                if jaro > self.config.CONF_THRESHOLD:  # check if confidence is greater than preset threshold\n",
    "\n",
    "                    # iterate over list of feature values that match the current modifier in domain knowledge\n",
    "                    for i in range(len(self.config.DOMAIN_KNOWLEDGE[feature][lookup])):\n",
    "\n",
    "                        vals[self.config.DOMAIN_KNOWLEDGE[feature][lookup][\n",
    "                            i]] += jaro  # accumulate evidence for lookup in hash\n",
    "\n",
    "                        if vals[self.config.DOMAIN_KNOWLEDGE[feature][lookup][i]] > max_val:\n",
    "\n",
    "                            max_val = vals[self.config.DOMAIN_KNOWLEDGE[feature][lookup][i]]  # update max confidence\n",
    "                            max_arg = [self.config.DOMAIN_KNOWLEDGE[feature][lookup][i]]  # update associated max argument\n",
    "\n",
    "                        elif vals[self.config.DOMAIN_KNOWLEDGE[feature][lookup][i]] == max_val:\n",
    "\n",
    "                            # accomodate for series of feature values that match the current modifier equally\n",
    "                            max_arg.append(self.config.DOMAIN_KNOWLEDGE[feature][lookup][i])\n",
    "\n",
    "        return max_arg if max_arg else None\n",
    "\n",
    "    def generate_features_rf_(self):\n",
    "\n",
    "        '''Generate Bag-of-words of Relevant Features on Most Recent Query\n",
    "        Current implementation supports suggestion of relevant features by relevance feedback (Rochio algorithm)\n",
    "        Current implementation also supports suggestion of relevant features by frequent itemsets\n",
    "        Defaulted to implementation of relevance feedback, with results cached for future reference\n",
    "        Oracle caches features fetched over time and recalculates their weights in the cooccurence hash by\n",
    "        time since last hit using an exponential decay\n",
    "        '''\n",
    "\n",
    "        qry_vector = defaultdict(float)\n",
    "        rterms, nrterms = [], []\n",
    "\n",
    "        # represents (term, features) matrix where weight for each (term, feature) is dependent on cooccurence strength\n",
    "        cooccurence_hash = self.config.COOCCURENCE_HASH.copy()\n",
    "\n",
    "        for term, steps in self.config.EXP_DECAY.items():\n",
    "            cooccurence_hash[term][term] *= self.config.DECAY * (\n",
    "                        np.e ** (-self.config.DECAY * steps))  # update weights by exp decay\n",
    "\n",
    "        for feat, term_wgts in cooccurence_hash.items():  # iterate over feature, weight pairs in cooccurence hash\n",
    "\n",
    "            if feat in self.config.MOST_RECENT_QUERY:  # construct query vector on terms in most recent query\n",
    "                qry_vector[feat] += 1\n",
    "                rterms.append(term_wgts)\n",
    "            else:\n",
    "                nrterms.append(term_wgts)  # construct list of nonrelevant terms for Rochio\n",
    "\n",
    "        reform = self.rochio_algo(qry_vector, rterms, nrterms, 1, 0.75,\n",
    "                                  0.15)  # compute reformulated query vector by Rochio\n",
    "        rterms = []  # reinitialize rterms to hold suggested terms to investigate\n",
    "\n",
    "        for term in cooccurence_hash:  # iterate over each term (key) in cooccurence hash\n",
    "            cos_sim = self.cosine_sim(reform,\n",
    "                                      cooccurence_hash[term])  # compute similarity of each vector in cooccurence_hash\n",
    "            rterms.append((term, cos_sim))  # append to rterms\n",
    "\n",
    "        rterms.sort(key=lambda x: x[1])  # sort rterms before inserting into cache\n",
    "\n",
    "        # only include features deemed relevant over a preset threshold\n",
    "        rterms = [term[0] for term in rterms if term[1] > self.config.RELEVANCE_FEEDBACK_THRESHOLD]\n",
    "\n",
    "        return rterms\n",
    "\n",
    "    def generate_queries_itemsets(self):\n",
    "\n",
    "        '''Return association rules from frequent itemsets analysis of relevant features to investigate'''\n",
    "\n",
    "        relim_input = itemmining.get_relim_input(self.config.ITEMSETS)\n",
    "        item_sets = itemmining.relim(relim_input, min_support=self.config.ASSOC_MIN_SUPPORT)  # generate frequent itemsets\n",
    "        rules = assocrules.mine_assoc_rules(item_sets, min_support=self.config.ASSOC_MIN_SUPPORT,\n",
    "                                            min_confidence=self.config.ASSOC_MIN_CONFIDENCE)  # generate association rules\n",
    "        rules.sort(key=lambda x: -1 * x[2] * x[3])  # sort rules before inserting into cache\n",
    "\n",
    "        return rules\n",
    "\n",
    "    def rochio_algo(self, qry_vector, rel_terms, nonrel_terms, a, B, y):\n",
    "\n",
    "        '''Relevance Feedback by Rochio Algorithm for Automated Query Suggestion\n",
    "        Extension of original algorithm by caching results for future reference\n",
    "        '''\n",
    "\n",
    "        rels, nonrels = defaultdict(float), defaultdict(float)\n",
    "        dr, dnr = len(rel_terms), len(nonrel_terms)\n",
    "\n",
    "        # iterate over relevant feature set\n",
    "        for rel_term in rel_terms:\n",
    "\n",
    "            if dr <= 0: break\n",
    "\n",
    "            # iterate over each (feature, weight) tuple in rels\n",
    "            for rel_key in rel_term:\n",
    "                # reweight each feature weight in relevant set\n",
    "                rels[rel_key] += (B / dr) * rel_term[rel_key]\n",
    "\n",
    "        for nonrel_term in nonrel_terms:  # iterate over nonrelevant feature set\n",
    "\n",
    "            if dnr <= 0: break\n",
    "\n",
    "            # iterate over each (feature, weight) tuple in nonrels\n",
    "            for nonrel_key in nonrel_term:\n",
    "                # reweight each feature weight in nonrelevant set\n",
    "                nonrels[nonrel_key] += (y / dnr) * nonrel_term[nonrel_key]\n",
    "\n",
    "        for term in qry_vector:\n",
    "            # reweight initial query vector by alpha\n",
    "            qry_vector[term] *= a\n",
    "\n",
    "            # initialize reformulated query vector\n",
    "        reform = defaultdict(float)\n",
    "\n",
    "        for text in [qry_vector, rels]:\n",
    "            for term in text:\n",
    "                reform[term] += text[term]\n",
    "\n",
    "        for nonrel_key in nonrels:\n",
    "\n",
    "            if nonrel_key in reform:\n",
    "\n",
    "                # offset reformulated query to drift from the nonrelevant set\n",
    "                reform[nonrel_key] -= nonrels[nonrel_key]\n",
    "\n",
    "                if reform[nonrel_key] < 0:\n",
    "                    # reset features with weights < 0 to 0 in reform\n",
    "                    reform.pop(nonrel_key, None)\n",
    "\n",
    "        # return reformulated query vector\n",
    "        return reform\n",
    "\n",
    "    def cosine_sim(self, vec1, vec2, vec1_norm=0.0, vec2_norm=0.0):\n",
    "\n",
    "        '''Return cosine similarity between two vectors'''\n",
    "\n",
    "        if not vec1_norm:\n",
    "            vec1_norm = sum(v * v for v in vec1.values())\n",
    "        if not vec2_norm:\n",
    "            vec2_norm = sum(v * v for v in vec2.values())\n",
    "\n",
    "        # save some time of iterating over the shorter vec\n",
    "        if len(vec1) > len(vec2):\n",
    "            vec1, vec2 = vec2, vec1\n",
    "\n",
    "        # calculate the inner product\n",
    "        inner_product = sum(vec1.get(term, 0) * vec2.get(term, 0) for term in vec1.keys())\n",
    "\n",
    "        return inner_product / np.sqrt(vec1_norm * vec2_norm)\n",
    "\n",
    "    def filter_reduction(self, f):\n",
    "        reduction = {\n",
    "            'by': '=>*(%by%)',\n",
    "            'on': '=>*(%by%)',\n",
    "            'when': '=>*(%when%)',\n",
    "            'where': '=>*(%where%)'\n",
    "        }\n",
    "\n",
    "        if f in reduction:\n",
    "            return reduction[f]\n",
    "\n",
    "    def parse_modules(self, qry):\n",
    "    \n",
    "        stopwords = set(nltk.corpus.stopwords.words('english'))\n",
    "        tokens = nltk.word_tokenize(qry)\n",
    "        candidates = defaultdict(set)\n",
    "        clookup = defaultdict(set)\n",
    "        result = set()\n",
    "\n",
    "        for token in tokens:\n",
    "\n",
    "            mkeys = config.module_hash.keys()\n",
    "\n",
    "            for mkey in mkeys:\n",
    "\n",
    "                jaro = jellyfish.jaro_distance(token, mkey)\n",
    "\n",
    "                if jaro > config.MODULE_PARSING_THRESHOLD:\n",
    "\n",
    "                    matches = config.module_hash[mkey]\n",
    "\n",
    "                    for match in matches:\n",
    "                        candidates[match].add(mkey)\n",
    "                        clookup[match].add(token)\n",
    "\n",
    "        for module in candidates:\n",
    "\n",
    "            mset = candidates[module]\n",
    "            keys = config.modules_reversed[module]\n",
    "\n",
    "            for key in keys:\n",
    "                combined = set()\n",
    "                keyset = key.split()\n",
    "                combined |= set(keyset)\n",
    "                combined &= mset\n",
    "\n",
    "                if len(combined)/len(keyset) > config.CONF_THRESHOLD:\n",
    "                    result.add(module)\n",
    "\n",
    "        clookup  = {k: v for k, v in clookup.items() if k in result}\n",
    "\n",
    "        return clookup\n",
    "\n",
    "    def parse_query(self):\n",
    "        tokens = nltk.word_tokenize(self.config.qry) # tokenize user query\n",
    "        pos_tag = nltk.pos_tag(tokens) # apply part of speech tagger\n",
    "        items = defaultdict(deque)\n",
    "        tagged, index = [], 0\n",
    "        param = ''\n",
    "        for i, term in enumerate(pos_tag):\n",
    "            if term[0].endswith(']'):\n",
    "                param += term[0]\n",
    "                tagged.append((param, 'NN'))\n",
    "                items[param].append(index)\n",
    "                index += 1\n",
    "                param = ''\n",
    "            elif term[0].startswith('['):\n",
    "                param = term[0]\n",
    "            elif param:\n",
    "                if param not in ['[','='] and not term[0].startswith('{'):\n",
    "                    param += ' '\n",
    "                param += term[0]\n",
    "            else:\n",
    "                tagged.append(pos_tag[i])\n",
    "                items[term[0]].append(index)\n",
    "                index += 1\n",
    "        \n",
    "        return tagged, items\n",
    "\n",
    "    def extract_entities(self, pos_tag):\n",
    "\n",
    "        actors, ind, verbs = [], [], []\n",
    "        prev = False\n",
    "\n",
    "        for i, tag in enumerate(pos_tag):  # enumerate over each element of the tagged list\n",
    "\n",
    "            tag = list(tag)\n",
    "\n",
    "            if self.config.is_prep(tag[1]) or tag[0] in self.config.keywords:  # check if token is a preposition or a keyword\n",
    "\n",
    "                actors.append(tag)\n",
    "                prev = False\n",
    "\n",
    "            elif self.config.is_genitive(tag) or self.config.is_verb(tag) or self.config.is_actor(tag[1]):\n",
    "\n",
    "                if self.config.is_genitive(tag) or self.config.is_verb(\n",
    "                        tag):  # check if tagged element is a possessive modifier or a verb phrase\n",
    "\n",
    "                    if self.config.is_genitive(tag):\n",
    "                        tag[0] = self.config.GENITIVE  # reset query token to be the genitive placeholder '->'\n",
    "                    elif self.config.is_verb(tag):\n",
    "                        verbs.append(i)  # add to verbs list\n",
    "\n",
    "                    actors.append(tag)  # add to actors list\n",
    "                    prev = False\n",
    "\n",
    "                else:\n",
    "\n",
    "                    if not prev or tag[0].startswith('['):\n",
    "                        actors.append(tag)\n",
    "                        ind.append(i)\n",
    "                    else:\n",
    "                        actors[len(actors) - 1][0] += ' ' + tag[0]  # concatenate noun phrases with adjacent NNP\n",
    "                    prev = False if tag[0].startswith('[') else True\n",
    "\n",
    "            else:\n",
    "                prev = False\n",
    "        \n",
    "        return actors, ind, verbs\n",
    "\n",
    "    def group_actor_entities(self, pos_tag, actors, items, ind, verbs):\n",
    "\n",
    "        # initialize actor index and verb index\n",
    "        a_ind, v_ind = 0, 0\n",
    "        prev = False\n",
    "\n",
    "        for i in range(len(ind)):\n",
    "\n",
    "            if a_ind < len(actors) and v_ind < len(verbs) and self.config.is_verb(actors[a_ind]):\n",
    "\n",
    "                index = verbs[v_ind] + 1\n",
    "\n",
    "                while index < len(pos_tag) and self.config.is_adv(pos_tag[index][1]):\n",
    "                    # group adverbs and verb phrases as a single entity\n",
    "                    actors[a_ind][0] += ' ' + pos_tag[index][0]\n",
    "                    index += 1\n",
    "\n",
    "                v_ind += 1\n",
    "\n",
    "            while a_ind < len(actors) and not self.config.is_actor(actors[a_ind][1]):\n",
    "                a_ind += 1\n",
    "            \n",
    "            if not actors[a_ind][0].startswith('['):\n",
    "                index = items[actors[a_ind][0].split()[0]].popleft() - 1\n",
    "\n",
    "                while ((a_ind < len(actors) and index >= 0)\n",
    "                       and self.config.is_desc(pos_tag[index][1])):\n",
    "                    # concatenate noun phrases with adjacent modifiers\n",
    "                    actors[a_ind][0] = pos_tag[index][0] + ' ' + actors[a_ind][0]\n",
    "                    index -= 1\n",
    "\n",
    "            a_ind += 1\n",
    "\n",
    "        negated = False\n",
    "        for i, tag in enumerate(actors):\n",
    "            if re.search(r'(?i)^NOT$', actors[i][0].lower()) is not None:\n",
    "                negated = True\n",
    "            if self.config.is_actor(tag[1]):\n",
    "                if negated:\n",
    "                    tag[0] = '(NOT{0})'.format(tag[0])\n",
    "                else:\n",
    "                    tag[0] = '({0})'.format(tag[0])  # wrap noun phrases in parenthesis for tagging\n",
    "                negated = False\n",
    "\n",
    "        # remove gerund and noun phrase modifers adjacent to the concatenated sets contructed above\n",
    "        actors[:] = [actors[i] for i in range(len(actors)) if\n",
    "                     (not ((i + 1) < len(actors) and\n",
    "                           (self.config.is_gerund(actors[i][1]) and\n",
    "                            self.config.is_actor(actors[i + 1][1]))) and\n",
    "                      re.search(r'(?i)^NOT$', actors[i][0].lower()) is None)]\n",
    "        return actors\n",
    "\n",
    "    def update_conjunctive_lookup(self, actors):\n",
    "        self.config.conjunctive = {}\n",
    "        latest_actor, conj = None, None\n",
    "        for i in range(len(actors)):\n",
    "            if self.config.is_actor(actors[i][1]):\n",
    "                current = re.findall(r'\\((.+?)\\)', actors[i][0])[0]\n",
    "                if current not in self.config.CORE and current not in self.config.FEATURE_DIST:\n",
    "                    if conj is not None:\n",
    "                        previous = re.findall(r'\\((.+?)\\)', latest_actor)[0]\n",
    "                        if previous in self.config.conjunctive:\n",
    "                            previous = self.config.conjunctive[previous][1]\n",
    "                        self.config.conjunctive[current] = (conj, previous)\n",
    "                        conj = None\n",
    "                    latest_actor = actors[i][0]\n",
    "            elif self.config.is_conj(actors[i][0].lower()) and latest_actor is not None:\n",
    "                # lookup relevant keyword\n",
    "                conj = self.config.keywords[actors[i][0].lower()]\n",
    "\n",
    "    def handle_query(self, actors):\n",
    "\n",
    "        prev_p, prev_a, prev_f, start_f, flag = False, False, False, False, -1  # set preposition, actor, filter flags\n",
    "        immediate_f, latest_feat_for_arg, conj, conj_filter = False, None, None, None\n",
    "        open_v = False  # set flag to check if currently parsing a verb phrase\n",
    "        extag = ''  # parsed result\n",
    "        pos_index = 0  # holds current position to edit result at in the finite state transduction\n",
    "        for i in range(len(actors)):\n",
    "            if not self.config.is_prep(actors[i][1]):\n",
    "                immediate_f = False\n",
    "            elif immediate_f:\n",
    "                continue\n",
    "            if start_f:\n",
    "                if self.config.is_prep(actors[i][1]):\n",
    "                    continue\n",
    "                else:\n",
    "                    extag += actors[i][0].lower() + ')'\n",
    "                    start_f = False\n",
    "                    continue\n",
    "            elif not extag and self.config.is_prep(actors[i][1]):\n",
    "                reduction = self.filter_reduction(actors[i][0])\n",
    "                if reduction is not None:\n",
    "                    extag += reduction + '('\n",
    "                    start_f = True\n",
    "                continue\n",
    "            if flag == 0:\n",
    "                if re.search(r'(?i)^of$', actors[i][0].lower()) is None:\n",
    "                    extag += '=>*(%filter%)(' + actors[i][0]\n",
    "                    flag, prev_f = 1, True\n",
    "                continue\n",
    "            if self.config.is_verb(actors[i]):  # check if token is a verb phrase\n",
    "                op = self.config.keywords[actors[i][0]]  # lookup relevant keyword\n",
    "                if not op:  # check if the token matches a preset token in keywords\n",
    "                    op = '=>*(%' + actors[i][0].lower() + '%)'  # substitute op with user specified action\n",
    "                pos_index = len(extag)\n",
    "                extag += op + '('\n",
    "                open_v = True  # set current parsing of verb phrase to true\n",
    "            elif self.config.is_conj(actors[i][0].lower()):  # check if token is a conjunction\n",
    "                conj = self.config.keywords[actors[i][0].lower()]  # lookup relevant keyword\n",
    "                pos_index = len(extag)\n",
    "                continue\n",
    "            elif self.config.is_genitive(actors[i]):  # check if token is a genitive phrase\n",
    "                pos_index = len(extag)\n",
    "                extag += actors[i][0]\n",
    "                if prev_f:\n",
    "                    continue\n",
    "            # check if preposition non-keyword preposition followers an actor\n",
    "            elif self.config.is_actor(actors[i][1]) and prev_p:\n",
    "                extag = extag[:pos_index] + actors[i][0] + extag[pos_index:]  # switch order of actor and preposition\n",
    "                prev_p = False  # set current parsing of preposition to false\n",
    "            elif self.config.is_prep(actors[i][1]) or actors[i][\n",
    "                0].lower() in self.config.keywords:  # check if token is a verb phrase\n",
    "                immediate_f = True\n",
    "                if conj is not None:\n",
    "                    conj_filter = self.config.keywords[actors[i][0].lower()]\n",
    "                    if not conj_filter:\n",
    "                        conj_filter = '=>*(%' + actors[i][0].lower() + '%)'\n",
    "                    continue\n",
    "                if open_v:\n",
    "                    extag += ')'  # close open verb tag\n",
    "                    open_v = False\n",
    "                # substitute with keyword representation encoded in domain knowledge\n",
    "                op = self.config.keywords[actors[i][0].lower()]\n",
    "                if not op:\n",
    "                    op = '=>*(%' + actors[i][\n",
    "                        0].lower() + '%)'  # substitute with user-specified token if not in keywords\n",
    "                if op in self.config.filters:\n",
    "                    if flag == 1:\n",
    "                        # concatenate keyword representation to extag\n",
    "                        extag = extag[:pos_index] + op + '(' + extag[pos_index:]\n",
    "                        pos_index += len(op) + 1\n",
    "                    else:\n",
    "                        pos_index = len(extag)\n",
    "                        extag += op + '('\n",
    "                    prev_p, prev_f = False, True  # not checking a prepositition but are checking a filter\n",
    "                    continue\n",
    "                extag = extag[:pos_index] + op + extag[pos_index:]  # update result\n",
    "                prev_p = True  # set parsing of preposition to true\n",
    "            else:\n",
    "                if flag == 1:  # flag marks prepositional clauses succeeding a verb phrase that it modifies\n",
    "                    extag = extag[:pos_index] + actors[i][0] + ')' + extag[pos_index:]\n",
    "                pos_index = len(extag)\n",
    "                if flag == 1:\n",
    "                    flag = -1\n",
    "                    if prev_f:\n",
    "                        extag += ')'\n",
    "                        prev_f = False\n",
    "                    elif (i + 1) < len(actors) and self.config.is_actor(actors[i + 1][1]):\n",
    "                        extag += '=>*(%filter%)('  # update result to accommodate parsed actions that require a parameter\n",
    "                        prev_f = True  # set parsing of filter to true\n",
    "                    continue\n",
    "                if conj is not None:\n",
    "                    extracted = re.findall(r'\\((.+?)\\)', actors[i][0])[0]\n",
    "                    extracted = self.match_(extracted)\n",
    "                    if (extracted is not None and latest_feat_for_arg is not None\n",
    "                            and extracted == latest_feat_for_arg):\n",
    "                        extag += conj\n",
    "                    else:\n",
    "                        if conj_filter is None or conj_filter == self.config.GENITIVE:\n",
    "                            conj_filter = '=>*(%filter%)('\n",
    "                            if flag != -1 or prev_f:\n",
    "                                conj_filter = ')' + conj_filter\n",
    "                        else:\n",
    "                            conj_filter = ')' + conj_filter + '('\n",
    "                        extag += conj_filter + actors[i][0]\n",
    "                        prev_f, conj_filter = True, None\n",
    "                        continue\n",
    "                extag += actors[i][0]  # update result\n",
    "                if prev_f and (i + 1) < len(actors) and re.search(r'(?i)^of$', actors[i + 1][0].lower()) is not None:\n",
    "                    flag = 0\n",
    "                    extag += ')'\n",
    "                    pos_index = len(extag)\n",
    "                    continue\n",
    "            if (i + 1) < len(actors) and self.config.is_conj(\n",
    "                    actors[i + 1][0]):  # ignore conjunctions that were handled above\n",
    "                latest_feat_for_arg = re.findall(r'\\((.+?)\\)', actors[i][0])[0]\n",
    "                latest_feat_for_arg = self.match_(latest_feat_for_arg)\n",
    "                continue\n",
    "            conj, conj_filter = None, None\n",
    "            if prev_f:  # check if modifiying a filtering substitution\n",
    "                pos_index = len(extag)\n",
    "                if ((i + 1) < len(actors) and\n",
    "                        (self.config.is_genitive(actors[i + 1]) or self.config.is_actor(actors[i + 1][1]))):\n",
    "                    if self.config.is_actor(actors[i + 1][1]):\n",
    "                        extag += ')=>*(%filter%)('  # accommodate action that requires a parameter\n",
    "                    continue\n",
    "                extag += ')'\n",
    "            prev_f = False\n",
    "        if open_v or prev_f:\n",
    "            extag += ')'\n",
    "        for s in self.config.subs:  # substitute tokens in the parsed result that match tokens in subs\n",
    "            extag = extag.replace(s, self.config.subs[s])\n",
    "\n",
    "        return extag\n",
    "\n",
    "    def generate_parse_lists(self, extag):\n",
    "\n",
    "        f_list, e_list = [], []  # initialize filters, entities lists\n",
    "        f_keys = [re.findall(r'\\(%(.+?)%\\)', f_key)[0] for f_key in self.config.filters]  # extract only content\n",
    "        f_keys = '(?:' + '|'.join(f_keys) + ')'\n",
    "        # pattern match against any filter in domain knowledge\n",
    "        pat = r'=>\\*\\(' + '%' + f_keys + '%' + r'\\)\\(\\(.+?\\)\\)\\)?'\n",
    "        pat_e = r'(' + pat + ')'\n",
    "        f_list.append(re.findall(pat, extag))\n",
    "\n",
    "        e_list = re.split(pat_e, extag)\n",
    "        f_list = list(itertools.chain.from_iterable(f_list))\n",
    "        e_list = [expr for expr in e_list if expr and re.search(pat_e, expr) is None]\n",
    "        e_list = [ent for expr in e_list for ent in re.split(r'(\\(.+?\\)\\)?)', expr) if ent]  # holds non-filter entities\n",
    "        pos_s, pos_e = -1, -1\n",
    "\n",
    "        for i in range(len(e_list)):\n",
    "\n",
    "            if e_list[i] == '=>*':  # mark actions\n",
    "                pos_s = i\n",
    "            elif re.search(r'\\(\\(.+\\)\\)', e_list[i]):\n",
    "                pos_e = i\n",
    "            if pos_s > 0 and pos_e > 0:\n",
    "                e_list[pos_s:(pos_e + 1)] = [''.join(e_list[pos_s:(pos_e + 1)])]\n",
    "                pos_s, pos_e = -1, -1\n",
    "\n",
    "        print('filtered: ', end='\\t');\n",
    "        print(f_list)\n",
    "        print('entity: ', end='\\t');\n",
    "        print(e_list)\n",
    "\n",
    "        return f_list, e_list\n",
    "\n",
    "    def run(self, qry):\n",
    "\n",
    "        '''Oracle's main driver\n",
    "        Instantiate an instance of this class and call this method on\n",
    "        a query to generate the Oracle's textual results and visualizations\n",
    "        @return: result (data frame), sample sizes (data frame),\n",
    "                 plot (Plotly object), rterms (relevance feedback suggestions) if parsing was successful\n",
    "        @return: None otherwise (sample size of 0 on query)\n",
    "        '''\n",
    "\n",
    "        self.config.qry = qry\n",
    "        self.config.clookup = self.parse_modules(self.config.qry)\n",
    "        print('\\nparsed modules: ' + str(self.config.clookup.keys()))\n",
    "        # preprocessing\n",
    "        pos_tag, items = self.parse_query()\n",
    "        actors, ind, verbs = self.extract_entities(pos_tag)\n",
    "        actors = self.group_actor_entities(pos_tag, actors, items, ind, verbs)\n",
    "        # update info on conjunctions for query\n",
    "        self.update_conjunctive_lookup(actors)\n",
    "        # return filter logic in Oracle syntax\n",
    "        extag = self.handle_query(actors)\n",
    "        # generate parsed filter and entity lists\n",
    "        f_list, e_list = self.generate_parse_lists(extag)\n",
    "        # apply sequential filter\n",
    "        self.config.filtered, b_feat = self.sequential_filter(f_list)\n",
    "        # apply sequential entity filter\n",
    "        # store results of query processing and generated visualization\n",
    "        result, sample_size, data, plot_type, rterms = self.sequential_entity(e_list, b_feat, actors)\n",
    "        return result, sample_size, data, plot_type, rterms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "desc_filename = os.path.join(os.getcwd(), '2017_updated_final.csv')\n",
    "config = Config(desc_filename)\n",
    "oracle = Oracle(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "qry = \"What is the strike rate of ranger pitchers by pitch type\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "parsed modules: dict_keys([<class 'oracle.modules.strikerate.StrikeRate'>])\n",
      "filtered: \t['=>*(%by%)((pitch type))']\n",
      "entity: \t['(ranger pitchers)', '->', '(strike rate)']\n",
      "\n",
      "feature association: most relevant feature for arg ( pitch type ) is pitch_type\n",
      "(team_id_pitcher, ranger pitchers): match_arg_to_feature_value\n",
      "(team_id_pitcher, ['texmlb']): filter_helper_\n",
      "\n",
      "self.config.filtered = self.config.filtered.apply(lambda g: (((g[g['team_id_pitcher'].isin(['texmlb'])]))))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.5/lib/python3.5/site-packages/ipykernel/__main__.py:200: FutureWarning: 'pitch_type' is both an index level and a column label.\n",
      "Defaulting to column, but this will raise an ambiguity error in a future version\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "strike rate\n",
      "entity: strike rate matched to module: <oracle.modules.strikerate.StrikeRate object at 0x13438a7b8>\n",
      "entity: strike rate matched to module: <oracle.modules.strikerate.StrikeRate object at 0x13438a7b8>\n",
      "entity: strike rate matched to module: <oracle.modules.strikerate.StrikeRate object at 0x13438a7b8>\n",
      "entity: strike rate matched to module: <oracle.modules.strikerate.StrikeRate object at 0x13438a7b8>\n",
      "entity: strike rate matched to module: <oracle.modules.strikerate.StrikeRate object at 0x13438a7b8>\n",
      "entity: strike rate matched to module: <oracle.modules.strikerate.StrikeRate object at 0x13438a7b8>\n",
      "entity: strike rate matched to module: <oracle.modules.strikerate.StrikeRate object at 0x13438a7b8>\n",
      "entity: strike rate matched to module: <oracle.modules.strikerate.StrikeRate object at 0x13438a7b8>\n",
      "entity: strike rate matched to module: <oracle.modules.strikerate.StrikeRate object at 0x13438a7b8>\n",
      "entity: strike rate matched to module: <oracle.modules.strikerate.StrikeRate object at 0x13438a7b8>\n",
      "here\n",
      "\n",
      "(Relevance Feedback) Investigate Features More Like This: ['team_id_pitcher', 'pitch_type']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# result is a pandas dataframe of the result data if query is not a flashcard, else is name of the generated .png file\n",
    "# sample_size is pandas dataframe\n",
    "# data is pandas dataframe where first column is all x values, second column is all y values\n",
    "# plot_type is string ('time-series', 'bar', 'png') indicating what plot to make\n",
    "# rterms is python list of relevance feedback query suggestions\n",
    "result, sample_size, data, plot_type, rterms = oracle.run(qry)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Output</th>\n",
       "      <th>name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.583520</td>\n",
       "      <td>CU</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.631826</td>\n",
       "      <td>FF</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.637341</td>\n",
       "      <td>FT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.676991</td>\n",
       "      <td>SI</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.604399</td>\n",
       "      <td>CH</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.590643</td>\n",
       "      <td>KC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.610782</td>\n",
       "      <td>SL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>UN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.639831</td>\n",
       "      <td>FS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.657604</td>\n",
       "      <td>FC</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Output name\n",
       "0  0.583520   CU\n",
       "0  0.631826   FF\n",
       "0  0.637341   FT\n",
       "0  0.676991   SI\n",
       "0  0.604399   CH\n",
       "0  0.590643   KC\n",
       "0  0.610782   SL\n",
       "0  1.000000   UN\n",
       "0  0.639831   FS\n",
       "0  0.657604   FC"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sample Sizes</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pitch_type</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>CH</th>\n",
       "      <td>2864</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CU</th>\n",
       "      <td>2682</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>FC</th>\n",
       "      <td>2170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>FF</th>\n",
       "      <td>7654</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>FS</th>\n",
       "      <td>236</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>FT</th>\n",
       "      <td>5672</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>KC</th>\n",
       "      <td>342</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SI</th>\n",
       "      <td>904</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SL</th>\n",
       "      <td>3209</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>UN</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Sample Sizes\n",
       "pitch_type              \n",
       "CH                  2864\n",
       "CU                  2682\n",
       "FC                  2170\n",
       "FF                  7654\n",
       "FS                   236\n",
       "FT                  5672\n",
       "KC                   342\n",
       "SI                   904\n",
       "SL                  3209\n",
       "UN                     2"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CH</td>\n",
       "      <td>0.604399</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>CU</td>\n",
       "      <td>0.583520</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>FC</td>\n",
       "      <td>0.657604</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>FF</td>\n",
       "      <td>0.631826</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>FS</td>\n",
       "      <td>0.639831</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>FT</td>\n",
       "      <td>0.637341</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>KC</td>\n",
       "      <td>0.590643</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>SI</td>\n",
       "      <td>0.676991</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>SL</td>\n",
       "      <td>0.610782</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>UN</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    x         y\n",
       "0  CH  0.604399\n",
       "1  CU  0.583520\n",
       "2  FC  0.657604\n",
       "3  FF  0.631826\n",
       "4  FS  0.639831\n",
       "5  FT  0.637341\n",
       "6  KC  0.590643\n",
       "7  SI  0.676991\n",
       "8  SL  0.610782\n",
       "9  UN  1.000000"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'bar'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plot_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['team_id_pitcher', 'pitch_type']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rterms"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
