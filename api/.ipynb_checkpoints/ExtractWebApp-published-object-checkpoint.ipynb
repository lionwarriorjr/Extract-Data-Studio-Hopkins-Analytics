{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from collections import defaultdict, deque\n",
    "import re\n",
    "import itertools\n",
    "import jellyfish\n",
    "import plotly\n",
    "import plotly.plotly as py\n",
    "import plotly.figure_factory as ff\n",
    "from plotly.graph_objs import *\n",
    "from pymining import itemmining, assocrules\n",
    "from functools import reduce\n",
    "from modules.module import Module\n",
    "from modules.strikerate import StrikeRate\n",
    "from modules.obp import OBP\n",
    "from modules.leadrunnerthird import LeadRunnerOnThird\n",
    "from modules.leadrunnersecond import LeadRunnerOnSecond\n",
    "from modules.leadrunnerfirst import LeadRunnerOnFirst\n",
    "from recommendations.recommendation import Recommendation, RecommendationItem\n",
    "from dknowledge import Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Oracle:\n",
    "    \n",
    "    def __init__(self, config):\n",
    "        self.config = config\n",
    "        \n",
    "    def is_number(self, s):\n",
    "        '''Return if a string can be parsed as a number'''\n",
    "        try:\n",
    "            float(s)\n",
    "            return True\n",
    "        except ValueError:\n",
    "            return False\n",
    "    \n",
    "    def user_input(self, args):\n",
    "    \n",
    "        '''Prompt user to clarify ambiguous tokens in the query'''\n",
    "      \n",
    "        print('What do you mean by ' + \"\\'\" + args + \"\\'\")\n",
    "    \n",
    "        feature = input('Relevant feature: ') # prompt for feature\n",
    "        value = input('Associated value: ') # prompt for value\n",
    "    \n",
    "        return (feature, [value])\n",
    "    \n",
    "    def identify_val_via_user(self, relevant, value):\n",
    "    \n",
    "        '''Given a feature determined to be relevant, prompt user for its associated value if ambiguous'''\n",
    "    \n",
    "        print('How does ' + \"\\'\" + value + \\\n",
    "          \"\\'\" + 'relate to the field ' + \"\\'\" + relevant + \"\\'\")\n",
    "        val = input('Relevant value: ') # prompt for value\n",
    "    \n",
    "        return [val]\n",
    "    \n",
    "    def substitute_conjunction(self, c):\n",
    "    \n",
    "        '''Return base Pandas substitution for conjunction'''\n",
    "    \n",
    "        result = {'OR': '|', 'AND': '&', 'NOT': '!'}\n",
    "        return result[c]\n",
    "    \n",
    "    def is_expr(self, entity):\n",
    "        '''Holds suite of pre built functions (averages, etc.)'''\n",
    "        return entity in config.module_keywords\n",
    "    \n",
    "    def module_filter(self, module):\n",
    "        module = module()\n",
    "        iset = range(len(config.filtered.index))\n",
    "        rset = module.execute(iset, config.module_tables['filtered'], False)\n",
    "        config.filtered = config.filtered.iloc[rset,:]\n",
    "    \n",
    "    def print_module_delegation(self, keyword, module):\n",
    "        print(\"delegating computation of '\" + keyword + \"' to module \" + str(module))\n",
    "    \n",
    "    def match_expr(self, entity, b_list, title, e_filters):\n",
    "    \n",
    "        '''driver for filtering on pre-built functions'''\n",
    "\n",
    "        plot, status = None, True\n",
    "        result, names, plot_l = '', [], []\n",
    "        module = None\n",
    "        config.module_tables = {'filtered': config.filtered, 'time_series': config.filtered}\n",
    "\n",
    "        if b_list is not None: # if b_list is not empty, perform a plot over distribution of feature\n",
    "            dist_plot = True\n",
    "            result = {}\n",
    "            for name, group in config.filtered: # iterate over each group\n",
    "                calc = None\n",
    "                if entity in config.module_keywords:\n",
    "                    module = config.module_keywords[entity]() if module is None else module\n",
    "                    calc = module.execute(config.filtered.indices[name], \n",
    "                                          config.module_tables['filtered'], True)\n",
    "                if calc is not None:\n",
    "                    if (entity not in e_filters or (calc.shape == (1,1) and\n",
    "                        (re.search(r'(?i)^over$', e_filters[entity][0]) is not None \n",
    "                         and calc.iloc[0,0] > float(e_filters[entity][1])) or\n",
    "                        (re.search(r'(?i)^under$', e_filters[entity][0]) is not None \n",
    "                         and calc.iloc[0,0] < float(e_filters[entity][1])))):\n",
    "                        names.append(name) # add to x labels list\n",
    "                        result[name] = calc\n",
    "                        if dist_plot:\n",
    "                            calc = calc.iloc[0,0]\n",
    "                            plot_l.append(calc) # add to y values list\n",
    "                        else:\n",
    "                            dist_plot = False\n",
    "\n",
    "            if plot_l and dist_plot: # if calculated results are returned, generate the plot\n",
    "                plot = pd.DataFrame({'x': names, 'y': plot_l})\n",
    "                #plot = self.automate_plot_by_(names, plot_l, entity, title, 'bar')\n",
    "                status = False # set time_series plot status to False\n",
    "\n",
    "        else: # otherwise plot time series by default\n",
    "\n",
    "            if entity in config.module_keywords:\n",
    "\n",
    "                module = config.module_keywords[entity]()\n",
    "                iset = range(len(config.filtered.index))\n",
    "                result = module.execute(iset, config.module_tables['filtered'], False)\n",
    "                config.time_series = config.filtered.groupby('Date') # group filtered dataset by date\n",
    "\n",
    "                if result.shape == (1,1):\n",
    "                    for name, group in config.time_series:\n",
    "                        module = config.module_keywords[entity]() if module is None else module\n",
    "                        calc = module.execute(config.time_series.indices[name], \n",
    "                                              config.module_tables['time_series'], False)\n",
    "                        calc = calc.iloc[0,0]\n",
    "                        if calc is not None:\n",
    "                            names.append(name)\n",
    "                            plot_l.append(calc)\n",
    "\n",
    "                if plot_l:\n",
    "                    # plot time series chart\n",
    "                    plot = pd.DataFrame({'x': names, 'y': plot_l})\n",
    "                    #plot = self.automate_plot_by_(names, plot_l, entity, title, 'scatter')\n",
    "\n",
    "        return result, plot, status\n",
    "    \n",
    "    def sequential_entity(self, e_list, b_feat, actors):\n",
    "    \n",
    "        '''Perform sequence of filter reductions on the entity list'''\n",
    "\n",
    "        response = str(' '.join([re.findall(r'^\\(?(.+[^)])\\)?', actor[0])[0] for actor in actors])).title()\n",
    "        criteria, added, expr = self.generate_criteria(e_list) # generate entity criteria (feature, [values])\n",
    "        result = None\n",
    "\n",
    "        eHash = {}\n",
    "        for eFilter in config.entityFilters:\n",
    "            eFilterIndex = config.qry.find(eFilter[0] + ' ' + eFilter[1])\n",
    "            minDiff = None\n",
    "            for e in expr:\n",
    "                eExprIndex = config.qry.find(e)\n",
    "                if minDiff is None or (eFilterIndex - eExprIndex) < minDiff:\n",
    "                    minDiff = eFilterIndex - eExprIndex\n",
    "                    eHash[eFilter] = e\n",
    "        \n",
    "        eHash = {v: k for k, v in eHash.items()}\n",
    "\n",
    "        for i in range(len(criteria)): # sequentially filter database on entity criteria\n",
    "\n",
    "            exec_str = '(' + self.perform_filter(config.FILTER, criteria[i], b_feat) + ')' # perform filter \n",
    "\n",
    "            if not b_feat:\n",
    "                exec_str = \"config.filtered = config.filtered[({0})]\".format(exec_str)\n",
    "            else:\n",
    "                exec_str = \"config.filtered = config.filtered.apply(lambda g: ({0}))\".format(exec_str)    \n",
    "            print('\\n' + exec_str)\n",
    "            exec(exec_str)\n",
    "\n",
    "            if b_feat:\n",
    "                config.filtered = config.filtered.groupby(b_feat)\n",
    "\n",
    "            feat = criteria[i][0]\n",
    "            config.MOST_RECENT_QUERY.add(feat) # cache this query as most recent\n",
    "            config.ITEMSETS[len(config.ITEMSETS)-1].add(feat) # add to frequent itemsets cache\n",
    "            config.EXP_DECAY[feat] = 1 # set distance of feature since last fetched to 1\n",
    "\n",
    "            for j in range(i, len(criteria)):\n",
    "                config.COOCCURENCE_HASH[feat][criteria[j][0]] += 1 # update cooccurrence hash\n",
    "\n",
    "        for e in expr:\n",
    "            result, plot, status = self.match_expr(e, b_feat, response, eHash) # sequentially execute all user defined actions\n",
    "\n",
    "        for feat in added: # restore original columns\n",
    "            del config.filtered[feat]\n",
    "            del config.DOMAIN_KNOWLEDGE[feat]\n",
    "\n",
    "        config.ITEMSETS[len(config.ITEMSETS)-1] = tuple(config.ITEMSETS[len(config.ITEMSETS)-1])\n",
    "        \n",
    "        sample_size = None\n",
    "        if result is not None:\n",
    "            if status: # output textual response\n",
    "                if result.shape == (1,1):\n",
    "                    result.columns = ['Output']\n",
    "                    sample_size = pd.DataFrame([config.filtered.shape[0]])\n",
    "            else:\n",
    "                appended = pd.DataFrame()\n",
    "                names = []\n",
    "                for name in result:\n",
    "                    if result[name].shape[1] == 1:\n",
    "                        result[name].columns = ['Output']\n",
    "                    appended = appended.append(result[name])\n",
    "                    names.append(name)\n",
    "                appended['name'] = names\n",
    "                appended.columns = ['Output','name']\n",
    "                result = appended\n",
    "                sample_size = config.filtered.size().to_frame()\n",
    "            sample_size.columns = ['Sample Sizes']\n",
    "        else:\n",
    "            print('\\n The output of your query resulted in a sample size of 0. \\\n",
    "                   Consider refining your query in the case that it was too specific.')\n",
    "            return None\n",
    "\n",
    "        rterms = self.generate_features_rf_()\n",
    "\n",
    "        if rterms:\n",
    "            print('\\n' + '(Relevance Feedback) Investigate Features More Like This: ' + str(rterms) + '\\n')\n",
    "            \n",
    "        return result, sample_size, plot, rterms\n",
    "        \n",
    "    def generate_criteria(self, e_list):\n",
    "    \n",
    "        '''Parse each entity token in the query into list of (feature, [value])'''\n",
    "\n",
    "        entities = []\n",
    "\n",
    "        for i in range(len(e_list)): # iterate over entity list\n",
    "            if e_list[i] == config.GENITIVE: \n",
    "                continue\n",
    "            entities.append(re.findall(r'^\\(?(.+[^)])\\)?', e_list[i])[0])\n",
    "\n",
    "        prev_feat = None\n",
    "        criteria, added, expr = [], [], []\n",
    "        rec_item = config.RECOMMENDATION.data[config.qry] # hash current query into RECOMMENDATIONS hash\n",
    "        conj = None\n",
    "\n",
    "        for entity in entities:\n",
    "\n",
    "            if self.is_expr(entity): # check if token is a pre-built expression\n",
    "                expr.append(entity)\n",
    "                continue\n",
    "            elif config.is_conj(entity): # check if token is a conjunction and tag it\n",
    "                conj = entity\n",
    "                continue\n",
    "\n",
    "            curr_feat = self.match_(entity) # extract relevant feature\n",
    "            start = config.qry.find(entity)\n",
    "            end = start + len(entity)\n",
    "            rec_item.index_hash[curr_feat] = (start, end) \n",
    "            config.RECOMMENDATION.data[config.qry] = rec_item\n",
    "\n",
    "            if curr_feat is None:\n",
    "                return self.user_input(entity) # prompt user to clarify input\n",
    "            else:\n",
    "                # extract associated value to relevant feature\n",
    "                val = self.match_arg_to_feature_value(curr_feat, entity) \n",
    "\n",
    "                while val is None:\n",
    "                    # prompt user if argument specification is ambiguous\n",
    "                    val = self.identify_val_via_user(curr_feat, val) \n",
    "\n",
    "                if criteria: # handle conjunctions in query\n",
    "                    if conj == 'OR': # union logic\n",
    "                        if criteria[len(criteria)-1][1] == curr_feat:\n",
    "                            criteria[len(criteria)-1][1][0] += '+' + '+'.join(val)\n",
    "                        else: \n",
    "                            pass\n",
    "                    elif conj == 'NOT': # negation logic\n",
    "                        criteria.extend([(curr_feat, ['!' + v_key]) for v_key in val])\n",
    "                    else:\n",
    "                        criteria.append((curr_feat, ['+'.join(val)]))\n",
    "                else: # intersection logic\n",
    "                    criteria.append((curr_feat, ['+'.join(val)]))\n",
    "                conj = None\n",
    "\n",
    "        features = [feat for feat in config.RECOMMENDATION.data[config.qry].index_hash]\n",
    "        features.sort() # sort features to specify common key\n",
    "        config.RECOMMENDATIONS[str(features)].append(config.RECOMMENDATION) # hash features in RECOMMENDATIONS\n",
    "\n",
    "        return criteria, added, expr\n",
    "\n",
    "    def sequential_filter(self, f_list):\n",
    "\n",
    "        '''Execute sequential filters over dataset on supplied filtering criteria'''\n",
    "\n",
    "        config.MOST_RECENT_QUERY = set() # reinitialize MOST_RECENT_QUERY\n",
    "        config.ITEMSETS.append(set()) # reinitialize ITEMSETS\n",
    "\n",
    "        for feat in config.EXP_DECAY:\n",
    "            config.EXP_DECAY[feat] += 1 # increment all features distance from being last fetched by 1\n",
    "\n",
    "        config.filtered = config.X.copy() # reinitialize filtered\n",
    "        config.time_series = config.filtered\n",
    "        config.module_tables = {'filtered': config.filtered, 'time_series': config.time_series}\n",
    "        filters = self.feature_assoc_filters_helper(f_list) # generate filtering criteria as [[(feature, [values])]]\n",
    "        grouped, b_list = None, []\n",
    "\n",
    "        for f_key in filters:\n",
    "            exec_str = ''\n",
    "            for i in range(len(f_key)): # iterate over each filtering criterion\n",
    "                f_, c_, conj_ = f_key[i][0], f_key[i][1], f_key[i][2] # extract filtering operation, criteria, join flag\n",
    "                if c_ is None: continue\n",
    "                feat = c_[0]\n",
    "                config.MOST_RECENT_QUERY.add(feat)\n",
    "                config.ITEMSETS[len(config.ITEMSETS)-1].add(feat) # update ITEMSETS\n",
    "                config.EXP_DECAY[feat] = 1 # set distance since being fetched to 1\n",
    "                for j in range(i, len(f_key)):\n",
    "                    config.COOCCURENCE_HASH[feat][f_key[j][1][0]] += 1 # update cooccurence hash\n",
    "                if not c_[1]:\n",
    "                    b_list.append(feat) # add to b_list if filtering criteria is over a feature's entire distribution\n",
    "                    continue\n",
    "                if conj_ is not None:\n",
    "                    conj_ = self.substitute_conjunction(conj_)\n",
    "                    exec_str += \" {0} \".format(conj_)\n",
    "                if c_[1] in config.module_keywords.values():\n",
    "                    self.print_module_delegation(feat, str(c_[1]))\n",
    "                    self.module_filter(c_[1])\n",
    "                else:\n",
    "                    exec_str += self.perform_filter(f_, c_, False)\n",
    "            if exec_str:\n",
    "                exec_str = '(' + exec_str + ')'\n",
    "                exec_str = \"config.filtered = config.filtered[({0})]\".format(exec_str)\n",
    "                print('\\n' + exec_str)\n",
    "                exec(exec_str)\n",
    "\n",
    "        if b_list:\n",
    "            grouped, b_feat = self.by_helper_([b_list, []])\n",
    "        else:\n",
    "            for module in config.clookup:\n",
    "                if module().set_module():\n",
    "                    keyword = list(module().get_lexicon())[0]\n",
    "                    self.print_module_delegation(keyword, str(module))\n",
    "                    self.module_filter(module)\n",
    "\n",
    "        return (config.filtered, None) if grouped is None else (grouped, b_feat)\n",
    "\n",
    "    def check_filters(self, fname):\n",
    "\n",
    "        max_conf, max_filt = 0, None\n",
    "\n",
    "        for entry in config.module_keywords:\n",
    "            conf_f = jellyfish.jaro_distance(fname, entry)\n",
    "            if conf_f > max_conf:\n",
    "                max_conf, max_filt = conf_f, config.module_keywords[entry]\n",
    "\n",
    "        if max_conf > config.CONF_THRESHOLD:\n",
    "            return max_filt\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "    def feature_assoc_filters_helper(self, filters):\n",
    "\n",
    "        '''Return list of (relevant feature, [associated values]) tuples to filter automatically'''\n",
    "\n",
    "        f_ = [] # initialize result list\n",
    "        featureHash = {}\n",
    "        config.RECOMMENDATION = Recommendation(config.qry) # initialize RECOMMENDATION to hash on current query\n",
    "        rec_item = RecommendationItem()\n",
    "        config.entityFilters, prev_numeric, numeric_token = [], [], None\n",
    "\n",
    "        for f_item in filters:\n",
    "\n",
    "            tokens = re.findall(r'\\(.+?\\)|AND|OR|NOT', f_item) # tokenize filters\n",
    "            tokens[0] = re.findall(r'%(.+)%', tokens[0])[0]\n",
    "            tokens[1] = tokens[1][1:]\n",
    "            tokens = [re.findall(r'\\((.+?)\\)', token)[0] if '(' in token else token for token in tokens]\n",
    "            prev_feat, relevant, val, cat, joined = None, None, None, '', []\n",
    "            i, inc, c_flag = 1, False, False\n",
    "            negated = None\n",
    "\n",
    "            cset = config.clookup.values()\n",
    "            cset = [k for ckey in cset for k in ckey]\n",
    "\n",
    "            while i < len(tokens): # iterate over each filter in the filtered list\n",
    "\n",
    "                token = tokens[i]\n",
    "                token = token.split()\n",
    "                count = 0\n",
    "\n",
    "                for t in token:\n",
    "                    for ckey in cset:\n",
    "                        jaro = jellyfish.jaro_distance(t, ckey)\n",
    "                        if jaro > config.CONF_THRESHOLD:\n",
    "                            count += 1\n",
    "\n",
    "                if count == len(token):\n",
    "                    f_.append([(tokens[0], None, None)])\n",
    "                    i += 1\n",
    "                    continue\n",
    "\n",
    "                is_filter = False\n",
    "\n",
    "                if tokens[i].startswith('NOT'): \n",
    "                    cat += tokens[i]\n",
    "                    negated = re.findall(r'NOT|.+', tokens[i])[1]\n",
    "                    relevant = self.match_(negated)\n",
    "                    if relevant is None:\n",
    "                        relevant, val = self.user_input(negated)\n",
    "\n",
    "                if negated is None:\n",
    "\n",
    "                    if tokens[0] in config.numericFilters:\n",
    "                        if not prev_numeric:\n",
    "                            config.entityFilters.append((tokens[0], tokens[i]))\n",
    "                            break\n",
    "                        else: \n",
    "                            relevant = prev_numeric.pop()\n",
    "                            val = [tokens[i]] # set argument value to match token if a numeric feature\n",
    "\n",
    "                    if relevant is None:\n",
    "                        relevant = self.match_(tokens[i]) # extract relevant feature for tokens[i]\n",
    "                        if relevant is None:\n",
    "                            filter_to_apply = self.check_filters(tokens[i])\n",
    "                            if filter_to_apply is None:\n",
    "                                relevant, val = self.user_input(tokens[i]) # prompt user if ambiguous\n",
    "                            else:\n",
    "                                is_filter = True\n",
    "                                f_.append([(tokens[0], (tokens[i], filter_to_apply), None)])\n",
    "                        if not is_filter:\n",
    "                            start = config.qry.find(tokens[i]) # start index stored before cached in RECOMMENDATION\n",
    "                            end = start + len(tokens[i]) # end index stored before cached in RECOMMENDATION\n",
    "                            rec_item.index_hash[relevant] = (start, end) \n",
    "\n",
    "                            # set current recommendation item as the value to RECOMMENDATION hashed on the current query\n",
    "                            config.RECOMMENDATION.data[config.qry] = rec_item\n",
    "\n",
    "                    if val is None and not is_filter:\n",
    "\n",
    "                        cat += tokens[i]\n",
    "                        joined.append(tokens[i])\n",
    "\n",
    "                        while (i+1) < len(tokens) and config.is_conj(tokens[i+1].lower()): # check if next token is conjunction\n",
    "                            # otherwise append the next feature value to the running result\n",
    "                            cat += tokens[i+1] + tokens[i+2]\n",
    "                            joined.append(tokens[i+2])\n",
    "                            i += 3\n",
    "\n",
    "                        for core_entity in config.CORE:\n",
    "                            if jellyfish.jaro_distance(core_entity, cat) > config.NAME_THRESHOLD:\n",
    "                                c_flag = True\n",
    "                                break\n",
    "\n",
    "                if not c_flag:\n",
    "\n",
    "                    if not is_filter:\n",
    "\n",
    "                        if val is None:\n",
    "\n",
    "                            # call subroutine to generate filter criteria for current (feature, [arguments]) pair\n",
    "                            result = self.generate_filter_criteria(cat, relevant)\n",
    "                            inserted = False\n",
    "\n",
    "                            if not result[1] or result[1][0] != 'is.numeric':\n",
    "                                for joinedEntry in joined:\n",
    "                                    if (joinedEntry in config.conjunctive and \n",
    "                                        config.conjunctive[joinedEntry][1] in featureHash):\n",
    "                                        stored = config.conjunctive[joinedEntry]\n",
    "                                        if not inserted:\n",
    "                                            f_[featureHash[stored[1]]].append((tokens[0], result, stored[0]))\n",
    "                                            inserted = True                                \n",
    "                                        featureHash[joinedEntry] = featureHash[stored[1]]\n",
    "\n",
    "                                if not inserted: \n",
    "                                    f_.append([(tokens[0], result, None)]) # append to result list\n",
    "                                    for joinedEntry in joined:    \n",
    "                                        featureHash[joinedEntry] = len(f_)-1\n",
    "\n",
    "                            else:\n",
    "                                numeric_token = cat\n",
    "                                prev_numeric.append(result[0])\n",
    "\n",
    "                        elif numeric_token is not None:\n",
    "                            if val[0] in config.conjunctive and config.conjunctive[val[0]][1] in featureHash:\n",
    "                                stored = config.conjunctive[val[0]]\n",
    "                                f_[featureHash[stored[1]]].append((tokens[0], (relevant, val), stored[0]))\n",
    "                                featureHash[val[0]] = featureHash[stored[1]]\n",
    "                            elif numeric_token in config.conjunctive and config.conjunctive[numeric_token][1] in featureHash:\n",
    "                                stored = config.conjunctive[numeric_token]\n",
    "                                f_[featureHash[stored[1]]].append((tokens[0], (relevant, val), stored[0]))\n",
    "                                featureHash[numeric_token] = featureHash[stored[1]]\n",
    "                            else:\n",
    "                                f_.append([(tokens[0], (relevant, val), None)]) # append to result list\n",
    "                                featureHash[val[0]] = len(f_)-1\n",
    "\n",
    "                            numeric_token = None\n",
    "\n",
    "                if not inc: \n",
    "                    i += 1\n",
    "\n",
    "                inc, c_flag, val, cat, joined = False, False, None, '', []\n",
    "\n",
    "        return f_\n",
    "\n",
    "\n",
    "    def perform_filter(self, f_key, c_key, b_list):\n",
    "\n",
    "        '''Perform relevant filter'''\n",
    "\n",
    "        result = {\n",
    "            'filter': lambda D: self.filter_helper_(c_key, b_list),\n",
    "            'by': lambda D: self.by_helper_(c_key),\n",
    "            'over': lambda D: self.over_helper_(c_key),\n",
    "            'under': lambda D: self.under_helper_(c_key),\n",
    "            'between': lambda D: self.between_helper_(c_key),\n",
    "            'except': lambda D: self.except_helper_(c_key),\n",
    "            'near': lambda D: self.near_helper_(c_key),\n",
    "            'until': lambda D: self.until_helper_(c_key),\n",
    "            'to': lambda D: self.to_helper_(c_key),\n",
    "            'after': lambda D: self.after_helper_(c_key),\n",
    "            'before': lambda D: self.before_helper_(c_key),\n",
    "            'against': lambda D: self.compare_helper_(c_key)\n",
    "        }[f_key](config.filtered)\n",
    "\n",
    "        return result;\n",
    "\n",
    "\n",
    "    def generate_filter_criteria(self, args, hint=None):\n",
    "\n",
    "        '''Return relevant (feature, [values]) tuple that matches argument'''\n",
    "\n",
    "        c_keys = ['AND', 'OR', 'NOT']\n",
    "        pat = '((?:' + '|'.join(c_keys) + '))'\n",
    "        c_list = re.split(pat, args) # split on conjunctions\n",
    "        c_list = [t for t in c_list if t]\n",
    "\n",
    "        if hint is None:\n",
    "            return self.user_input(args) # if ambiguous, prompt user to specify\n",
    "        else:\n",
    "            relevant = hint\n",
    "\n",
    "        print('\\nfeature association: most relevant feature for arg (', args, ') is ' + relevant)\n",
    "\n",
    "        if relevant in config.name_ids and args in config.IDENTIFIERS[relevant]:\n",
    "            return (relevant, [args])\n",
    "\n",
    "        if 'is.numeric' in list(config.DOMAIN_KNOWLEDGE[relevant].values())[0]:\n",
    "            if not self.is_number(args):\n",
    "                test_split = re.split(pat, args)\n",
    "                criteria = self.generate_filter_helper_(relevant, test_split)\n",
    "                return (relevant, criteria)\n",
    "            else:\n",
    "                return (relevant, ['is.numeric'])\n",
    "\n",
    "        featureDist = [config.FEATURE_DIST[feat] for feat in config.FEATURE_DIST \n",
    "                       if jellyfish.jaro_distance(args, feat) > config.CONF_THRESHOLD]\n",
    "        if featureDist:\n",
    "            return (featureDist[0], [])\n",
    "\n",
    "        criteria = self.generate_filter_helper_(relevant, c_list)\n",
    "\n",
    "        return (relevant, criteria)\n",
    "\n",
    "    def generate_filter_helper_(self, relevant, c_list):\n",
    "\n",
    "        criteria = []\n",
    "        conj, unionIndex = None, 0\n",
    "\n",
    "        for term in c_list:\n",
    "\n",
    "            if config.is_conj(term.lower()):\n",
    "                conj = term\n",
    "            else:\n",
    "                lookup = re.match('(.+)', term).group() # extract token\n",
    "                val = self.match_arg_to_feature_value(relevant, lookup) # look up associated filter criteria on feature\n",
    "                print('generate_criteria: ' + str(val))\n",
    "\n",
    "                while val is None:\n",
    "                    val = self.identify_val_via_user(relevant, val) # prompt user if associated value is ambiguous\n",
    "\n",
    "                if conj == 'NOT': # handle negation logic\n",
    "                    negated = ['!' + v_key for v_key in val]\n",
    "                    criteria.extend(negated)\n",
    "                    unionIndex = len(criteria) - len(negated)\n",
    "                elif criteria:\n",
    "                    if conj == 'OR' or len(val) > 1: # handle union logic\n",
    "                        for i in range(unionIndex, len(criteria)):\n",
    "                            criteria[i] += '+' + '+'.join(val)\n",
    "                        unionIndex = len(criteria)-1\n",
    "                    else:\n",
    "                        criteria.append('+'.join(val))\n",
    "                        unionIndex = len(criteria)-1\n",
    "                else: # handle intersection logic\n",
    "                    criteria.append('+'.join(val))\n",
    "                    unionIndex = len(criteria)-1    \n",
    "\n",
    "                conj = None\n",
    "\n",
    "        return criteria\n",
    "\n",
    "    def filter_helper_(self, c_key, b_list):\n",
    "\n",
    "        '''Perform filter on categorical feature'''\n",
    "\n",
    "        feature = c_key[0] # extract feature\n",
    "        args = c_key[1] # extract relevant values of feature to filter on\n",
    "        exec_str = ''\n",
    "        print('(' + str(feature) + ', ' + str(args) + '): filter_helper_')\n",
    "\n",
    "        if b_list: # handle automated filtering over distribution of a feature\n",
    "\n",
    "            for index, f_tok in enumerate(args): # iterate over each argument            \n",
    "\n",
    "                f_ = re.split(r'(?:\\+|!)', f_tok) # split by filters handled with conjunctive logic\n",
    "\n",
    "                if exec_str:\n",
    "                    exec_str += ' | '\n",
    "                if '!' in f_tok: # handle negation\n",
    "                    exec_str += \"(g[g[\\'\" + feature + \"\\'] != \\'\" + f_[1] + \"\\'])\"\n",
    "                else:\n",
    "                    union = [item for item in f_]\n",
    "                    exec_str += \"(g[g[\\'\" + feature + \"\\'].isin(\" + str(union) + \")])\"\n",
    "\n",
    "        else:\n",
    "\n",
    "            for index, f_tok in enumerate(args): # iterate over each argument\n",
    "\n",
    "                f_ = re.split(r'(?:\\+|!)', f_tok) # split on conjunctions            \n",
    "\n",
    "                if exec_str:\n",
    "                    exec_str += ' | '\n",
    "                if '!' in f_tok: # negation logic\n",
    "                    exec_str += \"(config.filtered[\\'\" + feature + \"\\'] != \\'\" + f_[1] + \"\\')\"\n",
    "                    continue     \n",
    "                exec_str += \"(config.filtered[\\'\" + feature + \"\\'] == \\'\" + f_[0] + \"\\')\"            \n",
    "\n",
    "                for i in range(1, len(f_)):\n",
    "                    exec_str += \" | (config.filtered[\\'\" + feature + \"\\'] == \\'\" + f_[i] + \"\\')\" # handle union logic\n",
    "\n",
    "        return exec_str\n",
    "\n",
    "    def by_helper_(self, c_key):\n",
    "\n",
    "        '''Filter database over distribution of a feature'''\n",
    "\n",
    "        features = c_key[0] # extract features\n",
    "        config.filtered = config.filtered.groupby(features) # group by features\n",
    "\n",
    "        return config.filtered, features\n",
    "\n",
    "    def over_helper_(self, c_key):\n",
    "\n",
    "        '''Filter database on values over a threshold for a feature'''\n",
    "\n",
    "        feature = c_key[0] # extract feature\n",
    "        args = c_key[1] # extract value to filter on\n",
    "        exec_str = \"(config.filtered[\\'\" + feature + \"\\'] > \" + args[0] + \")\"\n",
    "\n",
    "        return exec_str\n",
    "\n",
    "    def under_helper_(self, c_key):\n",
    "\n",
    "        '''Filter database on values under a threshold for a feature'''\n",
    "\n",
    "        feature = c_key[0] # extract feature\n",
    "        args = c_key[1] # extract value to filter on\n",
    "        exec_str = \"(config.filtered[\\'\" + feature + \"\\'] < \" + args[0] + \")\"\n",
    "\n",
    "        return exec_str\n",
    "\n",
    "    def between_helper_(self, c_key):\n",
    "\n",
    "        '''Filter database on values between two thresholds for a feature'''\n",
    "\n",
    "        feature = c_key[0] # extract feature\n",
    "        args = c_key[1] # extract value to filter on\n",
    "        left, right = args[0], args[1]\n",
    "        exec_str = \"(config.filtered[\\'\" + feature + \"\\'] > \" + left + \")\" \\\n",
    "                    \"& (config.filtered[\\'\" + feature + \"\\'] < \" + right + \")\"\n",
    "\n",
    "        return exec_str\n",
    "\n",
    "    def except_helper_(self, c_key):\n",
    "\n",
    "        '''Filter database on values of a categorical feature except for those specified in the argument'''\n",
    "\n",
    "        feature = c_key[0] # extract feature\n",
    "        args = c_key[1] # extract values to filter on\n",
    "        unique_vals = list(set(list(config.filtered[feature].unique())) - set([args]))\n",
    "        unique_vals = [x for x in unique_vals if x == x]\n",
    "        unique_vals = '+'.join(unique_vals)\n",
    "\n",
    "        return filter_helper_([feature, unique_vals], None)\n",
    "\n",
    "    def near_helper_(self, c_key):\n",
    "\n",
    "        '''Filter database on values within +/- 0.5 std of the argument on a numerical feature'''\n",
    "\n",
    "        feature = c_key[0] # extract relevant feature\n",
    "        args = c_key[1] # extract values to filter on\n",
    "        left = str(float(args[0]) - 0.5 * config.filtered[feature].std()) # left bound -0.5 std\n",
    "        right = str(float(args[1]) + 0.5 * config.filtered[feature].std()) # right bound +0.5 std\n",
    "\n",
    "        if left > right:\n",
    "            left, right = right, left\n",
    "\n",
    "        return between_helper_([feature, [left, right]])\n",
    "\n",
    "    def automate_plot_by_(self, x, y, entity, title, chart_type):\n",
    "\n",
    "        '''Return automated visualization relevant to queried features'''\n",
    "\n",
    "        font=dict(family='Courier New, monospace', size=15, color='#7f7f7f') # set font\n",
    "\n",
    "        xaxis=dict( # set x-axis plot attributes\n",
    "            titlefont=dict(\n",
    "                family='Courier New, monospace',\n",
    "                size=12,\n",
    "                color='#7f7f7f'\n",
    "            )\n",
    "        )\n",
    "\n",
    "        yaxis=dict( # set y-axis plot attributes\n",
    "            title = entity,\n",
    "            titlefont=dict(\n",
    "                family='Courier New, monospace',\n",
    "                size=12,\n",
    "                color='#7f7f7f'\n",
    "            )\n",
    "        )\n",
    "\n",
    "        if chart_type == 'bar':\n",
    "            data = [Bar(x=x, y=y)] # set data for bar chart chart\n",
    "        else:\n",
    "            data = [plotly.graph_objs.Scatter(x=x, y=y)] # set data for defaulted time series scatter\n",
    "            rangeselector=dict(\n",
    "                buttons=list([\n",
    "                    dict(count=1,\n",
    "                         label='1m',\n",
    "                         step='month',\n",
    "                         stepmode='backward'),\n",
    "                    dict(count=6,\n",
    "                         label='6m',\n",
    "                         step='month',\n",
    "                         stepmode='backward'),\n",
    "                    dict(step='all')\n",
    "                ])\n",
    "            )\n",
    "\n",
    "            xaxis['rangeselector'] = rangeselector # add range selector\n",
    "            xaxis['rangeslider'] = dict()\n",
    "\n",
    "        layout = plotly.graph_objs.Layout( # adjust plot layout\n",
    "            title=title,\n",
    "            font=font,\n",
    "            xaxis=xaxis,\n",
    "            yaxis=yaxis\n",
    "        )\n",
    "\n",
    "        fig = plotly.graph_objs.Figure(data=data, layout=layout) # store automated plot\n",
    "\n",
    "        return py.iplot(fig, filename='extract_bar') if chart_type == 'bar' else py.iplot(fig, filename='extract_scatter')        \n",
    "\n",
    "    def match_(self, args):\n",
    "\n",
    "        '''Return relevant feature to filter on'''\n",
    "\n",
    "        featureDist = [config.FEATURE_DIST[feat] for feat in config.FEATURE_DIST \n",
    "                       if jellyfish.jaro_distance(args, feat) > config.CONF_THRESHOLD]\n",
    "        if featureDist:\n",
    "            return featureDist[0]\n",
    "\n",
    "        found_batter, found_pitcher = False, False\n",
    "        for identifier in config.IDENTIFIERS:\n",
    "            if args in config.IDENTIFIERS[identifier]:\n",
    "                if identifier == config.name_ids[0]:\n",
    "                    found_batter = True\n",
    "                else:\n",
    "                    found_pitcher = True\n",
    "\n",
    "        if not (found_batter and found_pitcher):\n",
    "            if found_batter:\n",
    "                return (config.name_ids[0])\n",
    "            elif found_pitcher:\n",
    "                return (config.name_ids[1])\n",
    "        else:\n",
    "            return ((config.name_ids[0], [args]) \n",
    "                    if len(config.X[config.X[config.name_ids[0]] == args]) > len(config.X[config.X[config.name_ids[1]] == args]) \n",
    "                    else (config.name_ids[1]))\n",
    "\n",
    "        max_conf, max_feat = 0, ''\n",
    "\n",
    "        for entry in config.DOMAIN_KNOWLEDGE: # iterate over terms in the system's domain knowledge\n",
    "\n",
    "            if args in config.DOMAIN_KNOWLEDGE[entry]: # if term matches exactly return it\n",
    "                return entry\n",
    "\n",
    "            tokens = args.split() # otherwise split tokens and accumulate evidence of belonging to each feature\n",
    "            conf_f = 0 # confidence\n",
    "            skipped = 0 # tokens not considered in parsing of the feature\n",
    "\n",
    "            for token in tokens: # iterate over each token\n",
    "\n",
    "                if not self.is_number(token):\n",
    "\n",
    "                    curr = 0\n",
    "                    skipped += 1\n",
    "\n",
    "                    for desc in list(config.DOMAIN_KNOWLEDGE[entry].keys()): # iterate over each term in domain knowledge\n",
    "\n",
    "                        # compute string similarity of query vs term in DOMAIN_KNOWLEDGE\n",
    "                        curr = max(jellyfish.jaro_distance(desc, token), curr) # string similarity by jaro_distance\n",
    "\n",
    "                    conf_f += curr # accumulate confidence score for feature\n",
    "\n",
    "            conf_f = conf_f/skipped if skipped > 0 else 0 # scaled confidence level of feature match\n",
    "\n",
    "            if conf_f > max_conf: # update max confidence level and associated feature\n",
    "                max_conf, max_feat = conf_f, entry\n",
    "\n",
    "        if max_conf > config.CONF_THRESHOLD:\n",
    "            return max_feat # return result\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "    def match_arg_to_feature_value(self, feature, args):\n",
    "\n",
    "        '''Return list of relevant arguments that match feature value'''\n",
    "\n",
    "        print('(' + feature + ', ' + args + '): ' + 'match_arg_to_feature_value')\n",
    "\n",
    "        if feature in config.name_ids:\n",
    "            return [args]\n",
    "\n",
    "        unique_vals = list(set(list(itertools.chain.from_iterable(config.DOMAIN_KNOWLEDGE[feature].values()))))\n",
    "        vals = {key:0 for key in unique_vals} # initialize hash to accumulate evidence for each value of feature\n",
    "        tokens = args.split() # tokenize argument\n",
    "        max_val, max_arg = 0, []\n",
    "\n",
    "        for token in tokens: # iterate over each token\n",
    "\n",
    "            if self.is_number(token):\n",
    "                return [token]\n",
    "\n",
    "            for lookup in config.DOMAIN_KNOWLEDGE[feature]: # lookup relevant modifiers in domain knowledge\n",
    "\n",
    "                jaro = jellyfish.jaro_distance(token, lookup) # compute string similarity of modifier vs query token\n",
    "\n",
    "                # jaro normalized as a confidence between [0, 1]\n",
    "                if jaro > config.CONF_THRESHOLD: # check if confidence is greater than preset threshold\n",
    "\n",
    "                    # iterate over list of feature values that match the current modifier in domain knowledge\n",
    "                    for i in range(len(config.DOMAIN_KNOWLEDGE[feature][lookup])):\n",
    "\n",
    "                        vals[config.DOMAIN_KNOWLEDGE[feature][lookup][i]] += jaro # accumulate evidence for lookup in hash\n",
    "\n",
    "                        if vals[config.DOMAIN_KNOWLEDGE[feature][lookup][i]] > max_val:\n",
    "\n",
    "                            max_val = vals[config.DOMAIN_KNOWLEDGE[feature][lookup][i]] # update max confidence\n",
    "                            max_arg = [config.DOMAIN_KNOWLEDGE[feature][lookup][i]] # update associated max argument\n",
    "\n",
    "                        elif vals[config.DOMAIN_KNOWLEDGE[feature][lookup][i]] == max_val:\n",
    "\n",
    "                            # accomodate for series of feature values that match the current modifier equally \n",
    "                            max_arg.append(config.DOMAIN_KNOWLEDGE[feature][lookup][i])\n",
    "\n",
    "        return max_arg if max_arg else None\n",
    "    \n",
    "    def generate_features_rf_(self):\n",
    "    \n",
    "        '''Generate Bag-of-words of Relevant Features on Most Recent Query\n",
    "\n",
    "        Current implementation supports suggestion of relevant features by relevance feedback (Rochio algorithm)\n",
    "        Current implementation also supports suggestion of relevant features by frequent itemsets\n",
    "        Defaulted to implementation of relevance feedback, with results cached for future reference\n",
    "\n",
    "        Oracle caches features fetched over time and recalculates their weights in the cooccurence hash by\n",
    "        time since last hit using an exponential decay\n",
    "        '''\n",
    "\n",
    "        qry_vector = defaultdict(float)\n",
    "        rterms, nrterms = [], []\n",
    "\n",
    "        # represents (term, features) matrix where weight for each (term, feature) is dependent on cooccurence strength\n",
    "        cooccurence_hash = config.COOCCURENCE_HASH.copy()\n",
    "\n",
    "        for term, steps in config.EXP_DECAY.items():\n",
    "            cooccurence_hash[term][term] *= config.DECAY * (np.e ** (-config.DECAY * steps)) # update weights by exp decay\n",
    "\n",
    "        for feat, term_wgts in cooccurence_hash.items(): # iterate over feature, weight pairs in cooccurence hash\n",
    "\n",
    "            if feat in config.MOST_RECENT_QUERY: # construct query vector on terms in most recent query\n",
    "                qry_vector[feat] += 1\n",
    "                rterms.append(term_wgts)\n",
    "            else:\n",
    "                nrterms.append(term_wgts) # construct list of nonrelevant terms for Rochio\n",
    "\n",
    "        reform = self.rochio_algo(qry_vector, rterms, nrterms, 1, 0.75, 0.15) # compute reformulated query vector by Rochio\n",
    "        rterms = [] # reinitialize rterms to hold suggested terms to investigate\n",
    "\n",
    "        for term in cooccurence_hash: # iterate over each term (key) in cooccurence hash\n",
    "            cos_sim = self.cosine_sim(reform, cooccurence_hash[term]) # compute similarity of each vector in cooccurence_hash\n",
    "            rterms.append((term, cos_sim)) # append to rterms\n",
    "\n",
    "        rterms.sort(key=lambda x: x[1]) # sort rterms before inserting into cache\n",
    "\n",
    "        # only include features deemed relevant over a preset threshold\n",
    "        rterms = [term[0] for term in rterms if term[1] > config.RELEVANCE_FEEDBACK_THRESHOLD]\n",
    "\n",
    "        return rterms\n",
    "\n",
    "    def generate_queries_itemsets(self):\n",
    "\n",
    "        '''Return association rules from frequent itemsets analysis of relevant features to investigate'''\n",
    "\n",
    "        relim_input = itemmining.get_relim_input(config.ITEMSETS)\n",
    "        item_sets = itemmining.relim(relim_input, min_support=config.ASSOC_MIN_SUPPORT) # generate frequent itemsets\n",
    "        rules = assocrules.mine_assoc_rules(item_sets, min_support=config.ASSOC_MIN_SUPPORT, \n",
    "                                            min_confidence=config.ASSOC_MIN_CONFIDENCE) # generate association rules\n",
    "        rules.sort(key=lambda x: -1 * x[2] * x[3]) # sort rules before inserting into cache\n",
    "\n",
    "        return rules\n",
    "    \n",
    "    def rochio_algo(self, qry_vector, rel_terms, nonrel_terms, a, B, y):\n",
    "    \n",
    "        '''Relevance Feedback by Rochio Algorithm for Automated Query Suggestion\n",
    "\n",
    "        Extension of original algorithm by caching results for future reference\n",
    "        '''\n",
    "\n",
    "        rels, nonrels = defaultdict(float), defaultdict(float)\n",
    "        dr, dnr = len(rel_terms), len(nonrel_terms)\n",
    "\n",
    "        # iterate over relevant feature set\n",
    "        for rel_term in rel_terms: \n",
    "\n",
    "            if dr <= 0: break\n",
    "\n",
    "            # iterate over each (feature, weight) tuple in rels\n",
    "            for rel_key in rel_term: \n",
    "                # reweight each feature weight in relevant set\n",
    "                rels[rel_key] += (B / dr) * rel_term[rel_key] \n",
    "\n",
    "        for nonrel_term in nonrel_terms: # iterate over nonrelevant feature set\n",
    "\n",
    "            if dnr <= 0: break\n",
    "\n",
    "            # iterate over each (feature, weight) tuple in nonrels\n",
    "            for nonrel_key in nonrel_term: \n",
    "                # reweight each feature weight in nonrelevant set\n",
    "                nonrels[nonrel_key] += (y / dnr) * nonrel_term[nonrel_key]  \n",
    "\n",
    "        for term in qry_vector:\n",
    "            # reweight initial query vector by alpha\n",
    "            qry_vector[term] *= a \n",
    "\n",
    "        # initialize reformulated query vector\n",
    "        reform = defaultdict(float) \n",
    "\n",
    "        for text in [qry_vector, rels]:\n",
    "            for term in text:\n",
    "                reform[term] += text[term]\n",
    "\n",
    "        for nonrel_key in nonrels:\n",
    "\n",
    "            if nonrel_key in reform:\n",
    "\n",
    "                # offset reformulated query to drift from the nonrelevant set\n",
    "                reform[nonrel_key] -= nonrels[nonrel_key] \n",
    "\n",
    "                if reform[nonrel_key] < 0:\n",
    "                    # reset features with weights < 0 to 0 in reform\n",
    "                    reform.pop(nonrel_key, None) \n",
    "\n",
    "        # return reformulated query vector\n",
    "        return reform\n",
    "\n",
    "    def cosine_sim(self, vec1, vec2, vec1_norm = 0.0, vec2_norm = 0.0):\n",
    "\n",
    "        '''Return cosine similarity between two vectors'''\n",
    "\n",
    "        if not vec1_norm:\n",
    "            vec1_norm = sum(v * v for v in vec1.values())\n",
    "        if not vec2_norm:\n",
    "            vec2_norm = sum(v * v for v in vec2.values())\n",
    "\n",
    "        # save some time of iterating over the shorter vec\n",
    "        if len(vec1) > len(vec2):\n",
    "            vec1, vec2 = vec2, vec1\n",
    "\n",
    "        # calculate the inner product\n",
    "        inner_product = sum(vec1.get(term, 0) * vec2.get(term, 0) for term in vec1.keys())\n",
    "\n",
    "        return inner_product / np.sqrt(vec1_norm * vec2_norm)\n",
    "\n",
    "    def filter_reduction(self, f):\n",
    "        reduction = {\n",
    "            'by': '=>*(%by%)',\n",
    "            'on': '=>*(%by%)',\n",
    "            'when': '=>*(%when%)',\n",
    "            'where': '=>*(%where%)'\n",
    "        }\n",
    "\n",
    "        if f in reduction:\n",
    "            return reduction[f]\n",
    "    \n",
    "    def parse_modules(self, qry):\n",
    "    \n",
    "        stopwords = set(nltk.corpus.stopwords.words('english'))\n",
    "        tokens = nltk.word_tokenize(qry)\n",
    "        candidates = defaultdict(set)\n",
    "        clookup = defaultdict(set)\n",
    "        result = set()\n",
    "\n",
    "        for token in tokens:\n",
    "\n",
    "            mkeys = config.module_hash.keys()\n",
    "\n",
    "            for mkey in mkeys:\n",
    "\n",
    "                jaro = jellyfish.jaro_distance(token, mkey)\n",
    "\n",
    "                if jaro > config.MODULE_PARSING_THRESHOLD:\n",
    "\n",
    "                    matches = config.module_hash[mkey]\n",
    "\n",
    "                    for match in matches:\n",
    "                        candidates[match].add(mkey)\n",
    "                        clookup[match].add(token)\n",
    "\n",
    "        for module in candidates:\n",
    "\n",
    "            mset = candidates[module]\n",
    "            keys = config.modules_reversed[module]\n",
    "\n",
    "            for key in keys:\n",
    "                combined = set()\n",
    "                keyset = key.split()\n",
    "                combined |= set(keyset)\n",
    "                combined &= mset\n",
    "\n",
    "                if len(combined)/len(keyset) > config.CONF_THRESHOLD:\n",
    "                    result.add(module)\n",
    "\n",
    "        clookup  = {k: v for k, v in clookup.items() if k in result}\n",
    "\n",
    "        return clookup\n",
    "    \n",
    "    def parse_query(self):\n",
    "        tokens = nltk.word_tokenize(config.qry) # tokenize user query\n",
    "        pos_tag = nltk.pos_tag(tokens) # apply part of speech tagger\n",
    "        items = defaultdict(deque)\n",
    "        for i, term in enumerate(pos_tag):\n",
    "            items[term[0]].append(i)\n",
    "        return pos_tag, items\n",
    "    \n",
    "    def extract_entities(self, pos_tag):\n",
    "    \n",
    "        actors, ind, verbs = [], [], []\n",
    "        prev = False\n",
    "\n",
    "        for i, tag in enumerate(pos_tag): # enumerate over each element of the tagged list\n",
    "\n",
    "            tag = list(tag)\n",
    "\n",
    "            if config.is_prep(tag[1]) or tag[0] in config.keywords: # check if token is a preposition or a keyword\n",
    "\n",
    "                actors.append(tag)\n",
    "                prev = False    \n",
    "\n",
    "            elif config.is_genitive(tag) or config.is_verb(tag) or config.is_actor(tag[1]):\n",
    "\n",
    "                if config.is_genitive(tag) or config.is_verb(tag): # check if tagged element is a possessive modifier or a verb phrase\n",
    "\n",
    "                    if config.is_genitive(tag): \n",
    "                        tag[0] = config.GENITIVE # reset query token to be the genitive placeholder '->'\n",
    "                    elif config.is_verb(tag):\n",
    "                        verbs.append(i) # add to verbs list\n",
    "\n",
    "                    actors.append(tag) # add to actors list\n",
    "                    prev = False\n",
    "\n",
    "                else:\n",
    "\n",
    "                    if not prev:\n",
    "                        actors.append(tag)\n",
    "                        ind.append(i)\n",
    "                    else:\n",
    "                        actors[len(actors)-1][0] += ' ' + tag[0] # concatenate noun phrases with adjacent NNP\n",
    "\n",
    "                    prev = True\n",
    "\n",
    "            else:\n",
    "                prev = False\n",
    "\n",
    "        return actors, ind, verbs\n",
    "    \n",
    "    def group_actor_entities(self, pos_tag, actors, items, ind, verbs):\n",
    "    \n",
    "        # initialize actor index and verb index\n",
    "        a_ind, v_ind = 0, 0 \n",
    "        prev = False\n",
    "\n",
    "        for i in range(len(ind)):\n",
    "\n",
    "            if a_ind < len(actors) and v_ind < len(verbs) and config.is_verb(actors[a_ind]):\n",
    "\n",
    "                index = verbs[v_ind] + 1\n",
    "\n",
    "                while index < len(pos_tag) and config.is_adv(pos_tag[index][1]):\n",
    "                    # group adverbs and verb phrases as a single entity\n",
    "                    actors[a_ind][0] += ' ' + pos_tag[index][0] \n",
    "                    index += 1\n",
    "\n",
    "                v_ind += 1\n",
    "\n",
    "            while a_ind < len(actors) and not config.is_actor(actors[a_ind][1]):\n",
    "                a_ind += 1\n",
    "\n",
    "            index = items[actors[a_ind][0].split()[0]].popleft() - 1\n",
    "\n",
    "            while ((a_ind < len(actors) and index >= 0) \n",
    "                   and config.is_desc(pos_tag[index][1])):\n",
    "\n",
    "                # concatenate noun phrases with adjacent modifiers\n",
    "                actors[a_ind][0] = pos_tag[index][0] + ' ' + actors[a_ind][0]\n",
    "                index -= 1\n",
    "\n",
    "            a_ind += 1\n",
    "\n",
    "        negated = False\n",
    "        for i, tag in enumerate(actors):\n",
    "            if re.search(r'(?i)^NOT$', actors[i][0].lower()) is not None:\n",
    "                negated = True\n",
    "            if config.is_actor(tag[1]):\n",
    "                if negated:\n",
    "                    tag[0] = '(NOT{0})'.format(tag[0])\n",
    "                else:\n",
    "                    tag[0] = '({0})'.format(tag[0]) # wrap noun phrases in parenthesis for tagging\n",
    "                negated = False\n",
    "\n",
    "        # remove gerund and noun phrase modifers adjacent to the concatenated sets contructed above\n",
    "        actors[:] = [actors[i] for i in range(len(actors)) if \n",
    "                     (not ((i+1) < len(actors) and \n",
    "                           (config.is_gerund(actors[i][1]) and \n",
    "                            config.is_actor(actors[i+1][1]))) and\n",
    "                      re.search(r'(?i)^NOT$', actors[i][0].lower()) is None)]\n",
    "\n",
    "        return actors\n",
    "    \n",
    "    def update_conjunctive_lookup(self, actors):\n",
    "        config.conjunctive = {}\n",
    "        latest_actor, conj = None, None\n",
    "        for i in range(len(actors)):\n",
    "            if config.is_actor(actors[i][1]):\n",
    "                current = re.findall(r'\\((.+?)\\)', actors[i][0])[0]\n",
    "                if current not in config.CORE and current not in config.FEATURE_DIST:\n",
    "                    if conj is not None:\n",
    "                        previous = re.findall(r'\\((.+?)\\)', latest_actor)[0]\n",
    "                        if previous in config.conjunctive:\n",
    "                            previous = config.conjunctive[previous][1]\n",
    "                        config.conjunctive[current] = (conj, previous)\n",
    "                        conj = None\n",
    "                    latest_actor = actors[i][0]\n",
    "            elif config.is_conj(actors[i][0].lower()) and latest_actor is not None:\n",
    "                # lookup relevant keyword\n",
    "                conj = config.keywords[actors[i][0].lower()]\n",
    "                \n",
    "    def handle_query(self, actors):\n",
    "    \n",
    "        prev_p, prev_a, prev_f, start_f, flag = False, False, False, False, -1 # set preposition, actor, filter flags\n",
    "        immediate_f, latest_feat_for_arg, conj, conj_filter = False, None, None, None\n",
    "        open_v = False # set flag to check if currently parsing a verb phrase\n",
    "        extag = '' # parsed result\n",
    "        pos_index = 0 # holds current position to edit result at in the finite state transduction\n",
    "        for i in range(len(actors)):\n",
    "            if not config.is_prep(actors[i][1]):\n",
    "                immediate_f = False\n",
    "            elif immediate_f: continue\n",
    "            if start_f:\n",
    "                if config.is_prep(actors[i][1]): continue\n",
    "                else:\n",
    "                    extag += actors[i][0].lower() + ')'\n",
    "                    start_f = False\n",
    "                    continue\n",
    "            elif not extag and config.is_prep(actors[i][1]):\n",
    "                reduction = self.filter_reduction(actors[i][0])\n",
    "                if reduction is not None:\n",
    "                    extag += reduction + '('\n",
    "                    start_f = True\n",
    "                continue\n",
    "            if flag == 0:\n",
    "                if re.search(r'(?i)^of$', actors[i][0].lower()) is None:\n",
    "                    extag += '=>*(%filter%)(' + actors[i][0]\n",
    "                    flag, prev_f = 1, True\n",
    "                continue\n",
    "            if config.is_verb(actors[i]): # check if token is a verb phrase\n",
    "                op = config.keywords[actors[i][0]] # lookup relevant keyword\n",
    "                if not op: # check if the token matches a preset token in keywords\n",
    "                    op = '=>*(%' + actors[i][0].lower() + '%)' # substitute op with user specified action\n",
    "                pos_index = len(extag)\n",
    "                extag += op + '(' \n",
    "                open_v = True # set current parsing of verb phrase to true\n",
    "            elif config.is_conj(actors[i][0].lower()): # check if token is a conjunction\n",
    "                conj = config.keywords[actors[i][0].lower()] # lookup relevant keyword\n",
    "                pos_index = len(extag)\n",
    "                continue\n",
    "            elif config.is_genitive(actors[i]): # check if token is a genitive phrase\n",
    "                pos_index = len(extag)\n",
    "                extag += actors[i][0]\n",
    "                if prev_f: \n",
    "                    continue\n",
    "            # check if preposition non-keyword preposition followers an actor\n",
    "            elif config.is_actor(actors[i][1]) and prev_p:\n",
    "                extag = extag[:pos_index] + actors[i][0] + extag[pos_index:] # switch order of actor and preposition\n",
    "                prev_p = False # set current parsing of preposition to false\n",
    "            elif config.is_prep(actors[i][1]) or actors[i][0].lower() in config.keywords: # check if token is a verb phrase\n",
    "                immediate_f = True\n",
    "                if conj is not None:\n",
    "                    conj_filter = config.keywords[actors[i][0].lower()]\n",
    "                    if not conj_filter:\n",
    "                        conj_filter = '=>*(%' + actors[i][0].lower() + '%)'\n",
    "                    continue\n",
    "                if open_v: \n",
    "                    extag += ')' # close open verb tag\n",
    "                    open_v = False\n",
    "                # substitute with keyword representation encoded in domain knowledge\n",
    "                op = config.keywords[actors[i][0].lower()]\n",
    "                if not op:\n",
    "                    op = '=>*(%' + actors[i][0].lower() + '%)' # substitute with user-specified token if not in keywords\n",
    "                if op in config.filters:\n",
    "                    if flag == 1:\n",
    "                        # concatenate keyword representation to extag\n",
    "                        extag = extag[:pos_index] + op + '(' + extag[pos_index:]\n",
    "                        pos_index += len(op) + 1\n",
    "                    else:\n",
    "                        pos_index = len(extag)\n",
    "                        extag += op + '('\n",
    "                    prev_p, prev_f = False, True # not checking a prepositition but are checking a filter\n",
    "                    continue\n",
    "                extag = extag[:pos_index] + op + extag[pos_index:] # update result\n",
    "                prev_p = True # set parsing of preposition to true\n",
    "            else:\n",
    "                if flag == 1: # flag marks prepositional clauses succeeding a verb phrase that it modifies\n",
    "                    extag = extag[:pos_index] + actors[i][0] + ')' + extag[pos_index:]   \n",
    "                pos_index = len(extag)\n",
    "                if flag == 1:\n",
    "                    flag = -1\n",
    "                    if prev_f:\n",
    "                        extag += ')'\n",
    "                        prev_f = False\n",
    "                    elif (i+1) < len(actors) and config.is_actor(actors[i+1][1]):\n",
    "                        extag += '=>*(%filter%)(' # update result to accommodate parsed actions that require a parameter\n",
    "                        prev_f = True # set parsing of filter to true\n",
    "                    continue\n",
    "                if conj is not None:\n",
    "                    extracted = re.findall(r'\\((.+?)\\)', actors[i][0])[0]\n",
    "                    extracted = self.match_(extracted)\n",
    "                    if (extracted is not None and latest_feat_for_arg is not None \n",
    "                        and extracted == latest_feat_for_arg):\n",
    "                        extag += conj\n",
    "                    else:\n",
    "                        if conj_filter is None or conj_filter == config.GENITIVE:\n",
    "                            conj_filter = '=>*(%filter%)('\n",
    "                            if flag != -1 or prev_f:\n",
    "                                conj_filter = ')' + conj_filter\n",
    "                        else: \n",
    "                            conj_filter = ')' + conj_filter + '('\n",
    "                        extag += conj_filter + actors[i][0]\n",
    "                        prev_f, conj_filter = True, None\n",
    "                        continue\n",
    "                extag += actors[i][0] # update result\n",
    "                if prev_f and (i+1) < len(actors) and re.search(r'(?i)^of$', actors[i+1][0].lower()) is not None:\n",
    "                    flag = 0\n",
    "                    extag += ')'\n",
    "                    pos_index = len(extag)\n",
    "                    continue\n",
    "            if (i+1) < len(actors) and config.is_conj(actors[i+1][0]): # ignore conjunctions that were handled above\n",
    "                latest_feat_for_arg = re.findall(r'\\((.+?)\\)', actors[i][0])[0]\n",
    "                latest_feat_for_arg = self.match_(latest_feat_for_arg)\n",
    "                continue\n",
    "            conj, conj_filter = None, None\n",
    "            if prev_f: # check if modifiying a filtering substitution\n",
    "                pos_index = len(extag)\n",
    "                if ((i+1) < len(actors) and \n",
    "                    (config.is_genitive(actors[i+1]) or config.is_actor(actors[i+1][1]))):\n",
    "                    if config.is_actor(actors[i+1][1]):\n",
    "                        extag += ')=>*(%filter%)(' # accommodate action that requires a parameter\n",
    "                    continue\n",
    "                extag += ')'\n",
    "            prev_f = False\n",
    "        if open_v or prev_f:\n",
    "            extag += ')'\n",
    "        for s in config.subs: # substitute tokens in the parsed result that match tokens in subs\n",
    "            extag = extag.replace(s, config.subs[s])\n",
    "\n",
    "        return extag\n",
    "    \n",
    "    def generate_parse_lists(self, extag):\n",
    "\n",
    "        f_list, e_list = [], [] # initialize filters, entities lists\n",
    "        f_keys = [re.findall(r'\\(%(.+?)%\\)', f_key)[0] for f_key in config.filters] # extract only content\n",
    "        f_keys = '(?:' + '|'.join(f_keys) + ')'\n",
    "        # pattern match against any filter in domain knowledge\n",
    "        pat = r'=>\\*\\(' + '%' + f_keys + '%' + r'\\)\\(\\(.+?\\)\\)\\)?' \n",
    "        pat_e = r'(' + pat + ')'\n",
    "        f_list.append(re.findall(pat, extag))\n",
    "\n",
    "        e_list = re.split(pat_e, extag)\n",
    "        f_list = list(itertools.chain.from_iterable(f_list))\n",
    "        e_list = [expr for expr in e_list if expr and re.search(pat_e, expr) is None]\n",
    "        e_list = [ent for expr in e_list for ent in re.split(r'(\\(.+?\\)\\)?)', expr) if ent] # holds non-filter entities\n",
    "        pos_s, pos_e = -1, -1\n",
    "\n",
    "        for i in range(len(e_list)):\n",
    "\n",
    "            if e_list[i] == '=>*': # mark actions\n",
    "                pos_s = i    \n",
    "            elif re.search(r'\\(\\(.+\\)\\)', e_list[i]):\n",
    "                pos_e = i    \n",
    "            if pos_s > 0 and pos_e > 0:\n",
    "                e_list[pos_s:(pos_e+1)] = [''.join(e_list[pos_s:(pos_e+1)])]\n",
    "                pos_s, pos_e = -1, -1\n",
    "\n",
    "        print('filtered: ', end='\\t'); print(f_list)\n",
    "        print('entity: ', end='\\t'); print(e_list)\n",
    "\n",
    "        return f_list, e_list\n",
    "    \n",
    "    def run(self, qry):\n",
    "        \n",
    "        '''Oracle's main driver\n",
    "\n",
    "        Instantiate an instance of this class and call this method on\n",
    "        a query to generate the Oracle's textual results and visualizations\n",
    "        @return: result (data frame), sample sizes (data frame),\n",
    "                 plot (Plotly object), rterms (relevance feedback suggestions) if parsing was successful\n",
    "        @return: None otherwise (sample size of 0 on query)\n",
    "        '''\n",
    "        \n",
    "        config.qry = qry\n",
    "        config.clookup = self.parse_modules(config.qry)\n",
    "        print('\\nparsed modules: ' + str(config.clookup.keys()))\n",
    "        # preprocessing\n",
    "        pos_tag, items = self.parse_query()\n",
    "        actors, ind, verbs = self.extract_entities(pos_tag)\n",
    "        actors = self.group_actor_entities(pos_tag, actors, items, ind, verbs)\n",
    "        # update info on conjunctions for query\n",
    "        self.update_conjunctive_lookup(actors)\n",
    "        # return filter logic in Oracle syntax\n",
    "        extag = self.handle_query(actors)\n",
    "        # generate parsed filter and entity lists\n",
    "        f_list, e_list = self.generate_parse_lists(extag)\n",
    "        # apply sequential filter\n",
    "        config.filtered, b_feat = self.sequential_filter(f_list)\n",
    "        # apply sequential entity filter\n",
    "        # store results of query processing and generated visualization\n",
    "        result, sample_size, plot, rterms = self.sequential_entity(e_list, b_feat, actors)\n",
    "        if result is not None:\n",
    "            result = result.to_json()\n",
    "            sample_size = sample_size.to_json()\n",
    "            return result, sample_size, plot, rterms\n",
    "        else:\n",
    "            return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "username='smohan'\n",
    "api_key='UGKT8GURMTb64LlCxpBm'\n",
    "desc_filename = os.path.join(os.getcwd(), 'dataset/sample_query.csv')\n",
    "config = Config(username, api_key, desc_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query your data: what is the strike rate of ranger pitchers on fastballs\n"
     ]
    }
   ],
   "source": [
    "qry = input('Query your data: ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "parsed modules: dict_keys([<class 'modules.strikerate.StrikeRate'>])\n",
      "filtered: \t['=>*(%filter%)((fastballs))']\n",
      "entity: \t['(ranger pitchers)', '->', '(strike rate)']\n",
      "\n",
      "feature association: most relevant feature for arg ( fastballs ) is pitch_type\n",
      "(pitch_type, fastballs): match_arg_to_feature_value\n",
      "generate_criteria: ['FA', 'FF', 'FT', 'FC', 'FS']\n",
      "(pitch_type, ['FA+FF+FT+FC+FS']): filter_helper_\n",
      "\n",
      "config.filtered = config.filtered[(((config.filtered['pitch_type'] == 'FA') | (config.filtered['pitch_type'] == 'FF') | (config.filtered['pitch_type'] == 'FT') | (config.filtered['pitch_type'] == 'FC') | (config.filtered['pitch_type'] == 'FS')))]\n",
      "(team_id_pitcher, ranger pitchers): match_arg_to_feature_value\n",
      "(team_id_pitcher, ['texmlb']): filter_helper_\n",
      "\n",
      "config.filtered = config.filtered[(((config.filtered['team_id_pitcher'] == 'texmlb')))]\n",
      "\n",
      "(Relevance Feedback) Investigate Features More Like This: ['team_id_pitcher', 'pitch_type']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "oracle = Oracle(config)\n",
    "result, sample_size, plot, rterms = oracle.run(qry)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
